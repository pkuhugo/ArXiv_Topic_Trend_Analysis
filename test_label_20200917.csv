,id,submitter,authors,title_x,comments,journal-ref,doi,report-no,categories,license,abstract,versions,update_date,authors_parsed,year,month,day,title_y,umCitation,zscore,description,task,subtask,method,submethod,goal,subgoal,other1,other2
0,1705.02801,Palash Goyal,"Palash Goyal, Emilio Ferrara","Graph Embedding Techniques, Applications, and Performance: A Survey",Submitted to Knowledge Based Systems for review,"Knowledge Based Systems, Volume 151, 1 July 2018, Pages 78-94,
  2018",10.1016/j.knosys.2018.03.022,,cs.SI cs.LG physics.data-an,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Graphs, such as social networks, word co-occurrence networks, and
communication networks, occur naturally in various real-world applications.
Analyzing them yields insight into the structure of society, language, and
different patterns of communication. Many approaches have been proposed to
perform the analysis. Recently, methods which use the representation of graph
nodes in vector space have gained traction from the research community. In this
survey, we provide a comprehensive and structured analysis of various graph
embedding techniques proposed in the literature. We first introduce the
embedding task and its challenges such as scalability, choice of
dimensionality, and features to be preserved, and their possible solutions. We
then present three categories of approaches based on factorization methods,
random walks, and deep learning, with examples of representative algorithms in
each category and analysis of their performance on various tasks. We evaluate
these state-of-the-art methods on a few common datasets and compare their
performance against one another. Our analysis concludes by suggesting some
potential applications and future directions. We finally present the
open-source Python library we developed, named GEM (Graph Embedding Methods,
available at https://github.com/palash1992/GEM), which provides all presented
algorithms within a unified interface to foster and facilitate research on the
topic.
","[{'version': 'v1', 'created': 'Mon, 8 May 2017 09:47:38 GMT'}, {'version': 'v2', 'created': 'Tue, 9 May 2017 17:28:21 GMT'}, {'version': 'v3', 'created': 'Sun, 10 Dec 2017 17:24:19 GMT'}, {'version': 'v4', 'created': 'Fri, 22 Dec 2017 20:42:06 GMT'}]",2019-08-22,"[['Goyal', 'Palash', ''], ['Ferrara', 'Emilio', '']]",2019,8,22,"Graph Embedding Techniques, Applications, and Performance: A Survey",101,zscore: 6.597314,"Graphs, such as social networks, word co-occurrence networks, and communication networks, occur naturally in various real-world applications. Analyzing them yields insight into the structure of society, language, and different patterns of communication. Many approaches have been proposed to perform the analysis. Recently, methods which use the representation of graph nodes in vector space have gained traction from the research community. In this survey, we provide a comprehensive and structured analysis of various graph embedding techniques proposed in the literature. We first introduce the embedding task and its challenges such as scalability, choice of dimensionality, and features to be preserved, and their possible solutions. We then present three categories of approaches based on factorization methods, random walks, and deep learning, with examples of representative algorithms in each category and analysis of their performance on various tasks. We evaluate these state-of-the-art methods on a few common datasets and compare their performance against one another. Our analysis concludes by suggesting some potential applications and future directions. We finally present the open-source Python library we developed, named GEM (Graph Embedding Methods, available at https://github.com/palash1992/GEM), which provides all presented algorithms within a unified interface to foster and facilitate research on the topic.",Graphs,,Representation Learning,,Review,,,
1,1705.07204,Florian Tram\`er,"Florian Tram\`er, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow,
  Dan Boneh, Patrick McDaniel",Ensemble Adversarial Training: Attacks and Defenses,"22 pages, 5 figures, International Conference on Learning
  Representations (ICLR) 2018 (amended in April 2020 to include subsequent
  attacks that significantly reduced the robustness of our models)",,,,stat.ML cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Adversarial examples are perturbed inputs designed to fool machine learning
models. Adversarial training injects such examples into training data to
increase robustness. To scale this technique to large datasets, perturbations
are crafted using fast single-step methods that maximize a linear approximation
of the model's loss. We show that this form of adversarial training converges
to a degenerate global minimum, wherein small curvature artifacts near the data
points obfuscate a linear approximation of the loss. The model thus learns to
generate weak perturbations, rather than defend against strong ones. As a
result, we find that adversarial training remains vulnerable to black-box
attacks, where we transfer perturbations computed on undefended models, as well
as to a powerful novel single-step attack that escapes the non-smooth vicinity
of the input data via a small random step. We further introduce Ensemble
Adversarial Training, a technique that augments training data with
perturbations transferred from other models. On ImageNet, Ensemble Adversarial
Training yields models with strong robustness to black-box attacks. In
particular, our most robust model won the first round of the NIPS 2017
competition on Defenses against Adversarial Attacks. However, subsequent work
found that more elaborate black-box attacks could significantly enhance
transferability and reduce the accuracy of our models.
","[{'version': 'v1', 'created': 'Fri, 19 May 2017 21:56:43 GMT'}, {'version': 'v2', 'created': 'Tue, 30 May 2017 17:00:00 GMT'}, {'version': 'v3', 'created': 'Tue, 30 Jan 2018 18:47:39 GMT'}, {'version': 'v4', 'created': 'Sun, 22 Jul 2018 23:43:46 GMT'}, {'version': 'v5', 'created': 'Sun, 26 Apr 2020 22:20:25 GMT'}]",2020-04-28,"[['Tramèr', 'Florian', ''], ['Kurakin', 'Alexey', ''], ['Papernot', 'Nicolas', ''], ['Goodfellow', 'Ian', ''], ['Boneh', 'Dan', ''], ['McDaniel', 'Patrick', '']]",2020,4,28,Ensemble Adversarial Training: Attacks and Defenses,217,zscore: 7.748311,"Adversarial examples are perturbed inputs designed to fool machine learning models. Adversarial training injects such examples into training data to increase robustness. To scale this technique to large datasets, perturbations are crafted using fast single-step methods that maximize a linear approximation of the model's loss. We show that this form of adversarial training converges to a degenerate global minimum, wherein small curvature artifacts near the data points obfuscate a linear approximation of the loss. The model thus learns to generate weak perturbations, rather than defend against strong ones. As a result, we find that adversarial training remains vulnerable to black-box attacks, where we transfer perturbations computed on undefended models, as well as to a powerful novel single-step attack that escapes the non-smooth vicinity of the input data via a small random step. We further introduce Ensemble Adversarial Training, a technique that augments training data with perturbations transferred from other models. On ImageNet, Ensemble Adversarial Training yields models with strong robustness to black-box attacks. In particular, our most robust model won the first round of the NIPS 2017 competition on Defenses against Adversarial Attacks.",Image ,,Adversarial,Attacks,Exposing Weaknesses,,,
2,1705.07263,Nicholas Carlini,"Nicholas Carlini, David Wagner","Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection
  Methods",,,,,cs.LG cs.CR cs.CV,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural networks are known to be vulnerable to adversarial examples: inputs
that are close to natural inputs but classified incorrectly. In order to better
understand the space of adversarial examples, we survey ten recent proposals
that are designed for detection and compare their efficacy. We show that all
can be defeated by constructing new loss functions. We conclude that
adversarial examples are significantly harder to detect than previously
appreciated, and the properties believed to be intrinsic to adversarial
examples are in fact not. Finally, we propose several simple guidelines for
evaluating future proposed defenses.
","[{'version': 'v1', 'created': 'Sat, 20 May 2017 05:59:23 GMT'}, {'version': 'v2', 'created': 'Wed, 1 Nov 2017 04:07:05 GMT'}]",2017-11-02,"[['Carlini', 'Nicholas', ''], ['Wagner', 'David', '']]",2017,11,2,Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection   Methods,200,zscore: 7.150224,"Neural networks are known to be vulnerable to adversarial examples: inputs that are close to natural inputs but classified incorrectly. In order to better understand the space of adversarial examples, we survey ten recent proposals that are designed for detection and compare their efficacy. We show that all can be defeated by constructing new loss functions. We conclude that adversarial examples are significantly harder to detect than previously appreciated, and the properties believed to be intrinsic to adversarial examples are in fact not. Finally, we propose several simple guidelines for evaluating future proposed defenses.",Various,,GAN,,Exposing Weaknesses,,,
3,1705.07750,Joao Carreira,Joao Carreira and Andrew Zisserman,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset","Removed references to mini-kinetics dataset that was never made
  publicly available and repeated all experiments on the full Kinetics dataset",,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The paucity of videos in current action classification datasets (UCF-101 and
HMDB-51) has made it difficult to identify good video architectures, as most
methods obtain similar performance on existing small-scale benchmarks. This
paper re-evaluates state-of-the-art architectures in light of the new Kinetics
Human Action Video dataset. Kinetics has two orders of magnitude more data,
with 400 human action classes and over 400 clips per class, and is collected
from realistic, challenging YouTube videos. We provide an analysis on how
current architectures fare on the task of action classification on this dataset
and how much performance improves on the smaller benchmark datasets after
pre-training on Kinetics.
  We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on
2D ConvNet inflation: filters and pooling kernels of very deep image
classification ConvNets are expanded into 3D, making it possible to learn
seamless spatio-temporal feature extractors from video while leveraging
successful ImageNet architecture designs and even their parameters. We show
that, after pre-training on Kinetics, I3D models considerably improve upon the
state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0%
on UCF-101.
","[{'version': 'v1', 'created': 'Mon, 22 May 2017 13:57:53 GMT'}, {'version': 'v2', 'created': 'Thu, 20 Jul 2017 15:24:03 GMT'}, {'version': 'v3', 'created': 'Mon, 12 Feb 2018 17:10:11 GMT'}]",2018-02-13,"[['Carreira', 'Joao', ''], ['Zisserman', 'Andrew', '']]",2018,2,13,"Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset",245,zscore: 9.215116,"The paucity of videos in current action classification datasets (UCF-101 and HMDB-51) has made it difficult to identify good video architectures, as most methods obtain similar performance on existing small-scale benchmarks. This paper re-evaluates state-of-the-art architectures in light of the new Kinetics Human Action Video dataset. Kinetics has two orders of magnitude more data, with 400 human action classes and over 400 clips per class, and is collected from realistic, challenging YouTube videos. We provide an analysis on how current architectures fare on the task of action classification on this dataset and how much performance improves on the smaller benchmark datasets after pre-training on Kinetics.   We also introduce a new Two-Stream Inflated 3D ConvNet (I3D) that is based on 2D ConvNet inflation: filters and pooling kernels of very deep image classification ConvNets are expanded into 3D, making it possible to learn seamless spatio-temporal feature extractors from video while leveraging successful ImageNet architecture designs and even their parameters. We show that, after pre-training on Kinetics, I3D models considerably improve upon the state-of-the-art in action classification, reaching 80.9% on HMDB-51 and 98.0% on UCF-101.",Video,,Data,,Better/more difficult data,,,
4,1705.09064,Dongyu Meng,"Dongyu Meng, Hao Chen",MagNet: a Two-Pronged Defense against Adversarial Examples,"Accepted at the ACM Conference on Computer and Communications
  Security (CCS), 2017",,,,cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning has shown promising results on hard perceptual problems in
recent years. However, deep learning systems are found to be vulnerable to
small adversarial perturbations that are nearly imperceptible to human. Such
specially crafted perturbations cause deep learning systems to output incorrect
decisions, with potentially disastrous consequences. These vulnerabilities
hinder the deployment of deep learning systems where safety or security is
important. Attempts to secure deep learning systems either target specific
attacks or have been shown to be ineffective.
  In this paper, we propose MagNet, a framework for defending neural network
classifiers against adversarial examples. MagNet does not modify the protected
classifier or know the process for generating adversarial examples. MagNet
includes one or more separate detector networks and a reformer network.
Different from previous work, MagNet learns to differentiate between normal and
adversarial examples by approximating the manifold of normal examples. Since it
does not rely on any process for generating adversarial examples, it has
substantial generalization power. Moreover, MagNet reconstructs adversarial
examples by moving them towards the manifold, which is effective for helping
classify adversarial examples with small perturbation correctly. We discuss the
intrinsic difficulty in defending against whitebox attack and propose a
mechanism to defend against graybox attack. Inspired by the use of randomness
in cryptography, we propose to use diversity to strengthen MagNet. We show
empirically that MagNet is effective against most advanced state-of-the-art
attacks in blackbox and graybox scenarios while keeping false positive rate on
normal examples very low.
","[{'version': 'v1', 'created': 'Thu, 25 May 2017 06:49:57 GMT'}, {'version': 'v2', 'created': 'Mon, 11 Sep 2017 02:41:15 GMT'}]",2017-09-12,"[['Meng', 'Dongyu', ''], ['Chen', 'Hao', '']]",2017,9,12,MagNet: a Two-Pronged Defense against Adversarial Examples,118,zscore: 4.263524,"Deep learning has shown promising results on hard perceptual problems in recent years. However, deep learning systems are found to be vulnerable to small adversarial perturbations that are nearly imperceptible to human. Such specially crafted perturbations cause deep learning systems to output incorrect decisions, with potentially disastrous consequences. These vulnerabilities hinder the deployment of deep learning systems where safety or security is important. Attempts to secure deep learning systems either target specific attacks or have been shown to be ineffective.   In this paper, we propose MagNet, a framework for defending neural network classifiers against adversarial examples. MagNet does not modify the protected classifier or know the process for generating adversarial examples. MagNet includes one or more separate detector networks and a reformer network. Different from previous work, MagNet learns to differentiate between normal and adversarial examples by approximating the manifold of normal examples. Since it does not rely on any process for generating adversarial examples, it has substantial generalization power. Moreover, MagNet reconstructs adversarial examples by moving them towards the manifold, which is effective for helping classify adversarial examples with small perturbation correctly. We discuss the intrinsic difficulty in defending against whitebox attack and propose a mechanism to defend against graybox attack. Inspired by the use of randomness in cryptography, we propose to use diversity to strengthen MagNet. We show empirically that MagNet is effective against most advanced state-of-the-art attacks in blackbox and graybox scenarios while keeping false positive rate on normal examples very low.",Image ,,Adversarial,Attacks,Robustness,,,
5,1706.01427,Adam Santoro,"Adam Santoro, David Raposo, David G.T. Barrett, Mateusz Malinowski,
  Razvan Pascanu, Peter Battaglia, Timothy Lillicrap",A simple neural network module for relational reasoning,,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Relational reasoning is a central component of generally intelligent
behavior, but has proven difficult for neural networks to learn. In this paper
we describe how to use Relation Networks (RNs) as a simple plug-and-play module
to solve problems that fundamentally hinge on relational reasoning. We tested
RN-augmented networks on three tasks: visual question answering using a
challenging dataset called CLEVR, on which we achieve state-of-the-art,
super-human performance; text-based question answering using the bAbI suite of
tasks; and complex reasoning about dynamic physical systems. Then, using a
curated dataset called Sort-of-CLEVR we show that powerful convolutional
networks do not have a general capacity to solve relational questions, but can
gain this capacity when augmented with RNs. Our work shows how a deep learning
architecture equipped with an RN module can implicitly discover and learn to
reason about entities and their relations.
","[{'version': 'v1', 'created': 'Mon, 5 Jun 2017 17:17:18 GMT'}]",2017-06-06,"[['Santoro', 'Adam', ''], ['Raposo', 'David', ''], ['Barrett', 'David G. T.', ''], ['Malinowski', 'Mateusz', ''], ['Pascanu', 'Razvan', ''], ['Battaglia', 'Peter', ''], ['Lillicrap', 'Timothy', '']]",2017,6,6,A simple neural network module for relational reasoning,167,zscore: 5.646868,"Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.",Various,Reasoning,Deep Learning,Architectures (Relational),Better accuracy,,,
6,1706.02216,William L Hamilton,"William L. Hamilton, Rex Ying, Jure Leskovec",Inductive Representation Learning on Large Graphs,"Published in NIPS 2017; version with full appendix and minor
  corrections",,,,cs.SI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Low-dimensional embeddings of nodes in large graphs have proved extremely
useful in a variety of prediction tasks, from content recommendation to
identifying protein functions. However, most existing approaches require that
all nodes in the graph are present during training of the embeddings; these
previous approaches are inherently transductive and do not naturally generalize
to unseen nodes. Here we present GraphSAGE, a general, inductive framework that
leverages node feature information (e.g., text attributes) to efficiently
generate node embeddings for previously unseen data. Instead of training
individual embeddings for each node, we learn a function that generates
embeddings by sampling and aggregating features from a node's local
neighborhood. Our algorithm outperforms strong baselines on three inductive
node-classification benchmarks: we classify the category of unseen nodes in
evolving information graphs based on citation and Reddit post data, and we show
that our algorithm generalizes to completely unseen graphs using a multi-graph
dataset of protein-protein interactions.
","[{'version': 'v1', 'created': 'Wed, 7 Jun 2017 14:51:05 GMT'}, {'version': 'v2', 'created': 'Wed, 8 Nov 2017 01:45:25 GMT'}, {'version': 'v3', 'created': 'Tue, 10 Apr 2018 15:40:00 GMT'}, {'version': 'v4', 'created': 'Mon, 10 Sep 2018 14:26:58 GMT'}]",2018-09-11,"[['Hamilton', 'William L.', ''], ['Ying', 'Rex', ''], ['Leskovec', 'Jure', '']]",2018,9,11,Inductive Representation Learning on Large Graphs,178,zscore: 6.036766,"Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.",Graphs,,Representation Learning,,Better accuracy,,,
7,1706.02275,Ryan Lowe T.,"Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch",Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,,,,,cs.LG cs.AI cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We explore deep reinforcement learning methods for multi-agent domains. We
begin by analyzing the difficulty of traditional algorithms in the multi-agent
case: Q-learning is challenged by an inherent non-stationarity of the
environment, while policy gradient suffers from a variance that increases as
the number of agents grows. We then present an adaptation of actor-critic
methods that considers action policies of other agents and is able to
successfully learn policies that require complex multi-agent coordination.
Additionally, we introduce a training regimen utilizing an ensemble of policies
for each agent that leads to more robust multi-agent policies. We show the
strength of our approach compared to existing methods in cooperative as well as
competitive scenarios, where agent populations are able to discover various
physical and informational coordination strategies.
","[{'version': 'v1', 'created': 'Wed, 7 Jun 2017 17:35:00 GMT'}, {'version': 'v2', 'created': 'Wed, 21 Jun 2017 22:18:54 GMT'}, {'version': 'v3', 'created': 'Tue, 16 Jan 2018 23:37:25 GMT'}, {'version': 'v4', 'created': 'Sat, 14 Mar 2020 20:33:00 GMT'}]",2020-03-17,"[['Lowe', 'Ryan', ''], ['Wu', 'Yi', ''], ['Tamar', 'Aviv', ''], ['Harb', 'Jean', ''], ['Abbeel', 'Pieter', ''], ['Mordatch', 'Igor', '']]",2020,3,17,Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments,119,zscore: 3.878982,"We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multi-agent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.",Various,,Reinforcement Learning,,Better accuracy,,,
8,1706.02515,"G\""unter Klambauer","G\""unter Klambauer, Thomas Unterthiner, Andreas Mayr and Sepp
  Hochreiter",Self-Normalizing Neural Networks,9 pages (+ 93 pages appendix),Advances in Neural Information Processing Systems 30 (NIPS 2017),,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep Learning has revolutionized vision via convolutional neural networks
(CNNs) and natural language processing via recurrent neural networks (RNNs).
However, success stories of Deep Learning with standard feed-forward neural
networks (FNNs) are rare. FNNs that perform well are typically shallow and,
therefore cannot exploit many levels of abstract representations. We introduce
self-normalizing neural networks (SNNs) to enable high-level abstract
representations. While batch normalization requires explicit normalization,
neuron activations of SNNs automatically converge towards zero mean and unit
variance. The activation function of SNNs are ""scaled exponential linear units""
(SELUs), which induce self-normalizing properties. Using the Banach fixed-point
theorem, we prove that activations close to zero mean and unit variance that
are propagated through many network layers will converge towards zero mean and
unit variance -- even under the presence of noise and perturbations. This
convergence property of SNNs allows to (1) train deep networks with many
layers, (2) employ strong regularization, and (3) to make learning highly
robust. Furthermore, for activations not close to unit variance, we prove an
upper and lower bound on the variance, thus, vanishing and exploding gradients
are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning
repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with
standard FNNs and other machine learning methods such as random forests and
support vector machines. SNNs significantly outperformed all competing FNN
methods at 121 UCI tasks, outperformed all competing methods at the Tox21
dataset, and set a new record at an astronomy data set. The winning SNN
architectures are often very deep. Implementations are available at:
github.com/bioinf-jku/SNNs.
","[{'version': 'v1', 'created': 'Thu, 8 Jun 2017 11:14:24 GMT'}, {'version': 'v2', 'created': 'Sat, 10 Jun 2017 12:01:44 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Jun 2017 10:46:17 GMT'}, {'version': 'v4', 'created': 'Wed, 6 Sep 2017 13:33:53 GMT'}, {'version': 'v5', 'created': 'Thu, 7 Sep 2017 10:39:00 GMT'}]",2018-01-31,"[['Klambauer', 'Günter', ''], ['Unterthiner', 'Thomas', ''], ['Mayr', 'Andreas', ''], ['Hochreiter', 'Sepp', '']]",2018,1,31,Self-Normalizing Neural Networks,209,zscore: 7.223698,"Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ""scaled exponential linear units"" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.",Various,,Deep Learning,Architecture (SNN),Better accuracy,,,
9,1706.02677,Ross Girshick,"Priya Goyal, Piotr Doll\'ar, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, Kaiming He","Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",Tech report (v2: correct typos),,,,cs.CV cs.DC cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning thrives with large neural networks and large datasets. However,
larger networks and larger datasets result in longer training times that impede
research and development progress. Distributed synchronous SGD offers a
potential solution to this problem by dividing SGD minibatches over a pool of
parallel workers. Yet to make this scheme efficient, the per-worker workload
must be large, which implies nontrivial growth in the SGD minibatch size. In
this paper, we empirically show that on the ImageNet dataset large minibatches
cause optimization difficulties, but when these are addressed the trained
networks exhibit good generalization. Specifically, we show no loss of accuracy
when training with large minibatch sizes up to 8192 images. To achieve this
result, we adopt a hyper-parameter-free linear scaling rule for adjusting
learning rates as a function of minibatch size and develop a new warmup scheme
that overcomes optimization challenges early in training. With these simple
techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of
8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using
commodity hardware, our implementation achieves ~90% scaling efficiency when
moving from 8 to 256 GPUs. Our findings enable training visual recognition
models on internet-scale data with high efficiency.
","[{'version': 'v1', 'created': 'Thu, 8 Jun 2017 16:51:53 GMT'}, {'version': 'v2', 'created': 'Mon, 30 Apr 2018 21:53:41 GMT'}]",2018-05-02,"[['Goyal', 'Priya', ''], ['Dollár', 'Piotr', ''], ['Girshick', 'Ross', ''], ['Noordhuis', 'Pieter', ''], ['Wesolowski', 'Lukasz', ''], ['Kyrola', 'Aapo', ''], ['Tulloch', 'Andrew', ''], ['Jia', 'Yangqing', ''], ['He', 'Kaiming', '']]",2018,5,2,"Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour",266,zscore: 9.321866,"Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves ~90% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.",Image ,,Learning,Aspects of DL,Faster,,,
10,1706.05098,Sebastian Ruder,Sebastian Ruder,An Overview of Multi-Task Learning in Deep Neural Networks,"14 pages, 8 figures",,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-task learning (MTL) has led to successes in many applications of
machine learning, from natural language processing and speech recognition to
computer vision and drug discovery. This article aims to give a general
overview of MTL, particularly in deep neural networks. It introduces the two
most common methods for MTL in Deep Learning, gives an overview of the
literature, and discusses recent advances. In particular, it seeks to help ML
practitioners apply MTL by shedding light on how MTL works and providing
guidelines for choosing appropriate auxiliary tasks.
","[{'version': 'v1', 'created': 'Thu, 15 Jun 2017 21:38:12 GMT'}]",2017-06-19,"[['Ruder', 'Sebastian', '']]",2017,6,19,An Overview of Multi-Task Learning in Deep Neural Networks,120,zscore: 3.643814,"Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",Various,,Learning,MTL,Review,,,
11,1706.07979,Gr\'egoire Montavon,"Gr\'egoire Montavon, Wojciech Samek, Klaus-Robert M\""uller",Methods for Interpreting and Understanding Deep Neural Networks,"14 pages, 10 figures",,10.1016/j.dsp.2017.10.011,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper provides an entry point to the problem of interpreting a deep
neural network model and explaining its predictions. It is based on a tutorial
given at ICASSP 2017. It introduces some recently proposed techniques of
interpretation, along with theory, tricks and recommendations, to make most
efficient use of these techniques on real data. It also discusses a number of
practical applications.
","[{'version': 'v1', 'created': 'Sat, 24 Jun 2017 16:25:06 GMT'}]",2017-11-15,"[['Montavon', 'Grégoire', ''], ['Samek', 'Wojciech', ''], ['Müller', 'Klaus-Robert', '']]",2017,11,15,Methods for Interpreting and Understanding Deep Neural Networks,115,zscore: 5.544553,"This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.",General,,Interpretability,,Evaluation,,,
12,1706.08498,Matus Telgarsky,"Peter Bartlett, Dylan J. Foster, Matus Telgarsky",Spectrally-normalized margin bounds for neural networks,"Comparison to arXiv v1: 1-norm in main bound refined to
  (2,1)-group-norm. Comparison to NIPS camera ready: typo fixes",,,,cs.LG cs.NE stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper presents a margin-based multiclass generalization bound for neural
networks that scales with their margin-normalized ""spectral complexity"": their
Lipschitz constant, meaning the product of the spectral norms of the weight
matrices, times a certain correction factor. This bound is empirically
investigated for a standard AlexNet network trained with SGD on the mnist and
cifar10 datasets, with both original and random labels; the bound, the
Lipschitz constants, and the excess risks are all in direct correlation,
suggesting both that SGD selects predictors whose complexity scales with the
difficulty of the learning task, and secondly that the presented bound is
sensitive to this complexity.
","[{'version': 'v1', 'created': 'Mon, 26 Jun 2017 17:43:48 GMT'}, {'version': 'v2', 'created': 'Tue, 5 Dec 2017 06:08:38 GMT'}]",2017-12-06,"[['Bartlett', 'Peter', ''], ['Foster', 'Dylan J.', ''], ['Telgarsky', 'Matus', '']]",2017,12,6,Spectrally-normalized margin bounds for neural networks,99,zscore: 5.305894,"This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized ""spectral complexity"": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.",Theoretical,,Learning,"Generalization, Aspects of DL",Evaluation,,,
13,1706.08500,Martin Heusel,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
  Sepp Hochreiter","GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash
  Equilibrium",Implementations are available at: https://github.com/bioinf-jku/TTUR,Advances in Neural Information Processing Systems 30 (NIPS 2017),,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative Adversarial Networks (GANs) excel at creating realistic images
with complex models for which maximum likelihood is infeasible. However, the
convergence of GAN training has still not been proved. We propose a two
time-scale update rule (TTUR) for training GANs with stochastic gradient
descent on arbitrary GAN loss functions. TTUR has an individual learning rate
for both the discriminator and the generator. Using the theory of stochastic
approximation, we prove that the TTUR converges under mild assumptions to a
stationary local Nash equilibrium. The convergence carries over to the popular
Adam optimization, for which we prove that it follows the dynamics of a heavy
ball with friction and thus prefers flat minima in the objective landscape. For
the evaluation of the performance of GANs at image generation, we introduce the
""Fr\'echet Inception Distance"" (FID) which captures the similarity of generated
images to real ones better than the Inception Score. In experiments, TTUR
improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP)
outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN
Bedrooms, and the One Billion Word Benchmark.
","[{'version': 'v1', 'created': 'Mon, 26 Jun 2017 17:45:23 GMT'}, {'version': 'v2', 'created': 'Tue, 27 Jun 2017 15:06:08 GMT'}, {'version': 'v3', 'created': 'Wed, 28 Jun 2017 16:36:56 GMT'}, {'version': 'v4', 'created': 'Thu, 13 Jul 2017 09:15:29 GMT'}, {'version': 'v5', 'created': 'Wed, 8 Nov 2017 16:25:21 GMT'}, {'version': 'v6', 'created': 'Fri, 12 Jan 2018 14:05:44 GMT'}]",2018-01-31,"[['Heusel', 'Martin', ''], ['Ramsauer', 'Hubert', ''], ['Unterthiner', 'Thomas', ''], ['Nessler', 'Bernhard', ''], ['Hochreiter', 'Sepp', '']]",2018,1,31,GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash   Equilibrium,132,zscore: 7.234329,"Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the ""Fr\'echet Inception Distance"" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.",Image ,,GAN,,Better accuracy,,,
14,1706.08947,Behnam Neyshabur,"Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, Nathan
  Srebro",Exploring Generalization in Deep Learning,"19 pages, 8 figures",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With a goal of understanding what drives generalization in deep networks, we
consider several recently suggested explanations, including norm-based control,
sharpness and robustness. We study how these measures can ensure
generalization, highlighting the importance of scale normalization, and making
a connection between sharpness and PAC-Bayes theory. We then investigate how
well the measures explain different observed phenomena.
","[{'version': 'v1', 'created': 'Tue, 27 Jun 2017 17:20:06 GMT'}, {'version': 'v2', 'created': 'Thu, 6 Jul 2017 17:10:40 GMT'}]",2017-07-07,"[['Neyshabur', 'Behnam', ''], ['Bhojanapalli', 'Srinadh', ''], ['McAllester', 'David', ''], ['Srebro', 'Nathan', '']]",2017,7,7,Exploring Generalization in Deep Learning,84,zscore: 4.477013,"With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.",General,,Learning,Aspects of DL,Evaluation,,,
15,1706.10295,Charles Blundell,"Meire Fortunato, Mohammad Gheshlaghi Azar, Bilal Piot, Jacob Menick,
  Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis Hassabis, Olivier
  Pietquin, Charles Blundell, Shane Legg",Noisy Networks for Exploration,ICLR 2018,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce NoisyNet, a deep reinforcement learning agent with parametric
noise added to its weights, and show that the induced stochasticity of the
agent's policy can be used to aid efficient exploration. The parameters of the
noise are learned with gradient descent along with the remaining network
weights. NoisyNet is straightforward to implement and adds little computational
overhead. We find that replacing the conventional exploration heuristics for
A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively)
with NoisyNet yields substantially higher scores for a wide range of Atari
games, in some cases advancing the agent from sub to super-human performance.
","[{'version': 'v1', 'created': 'Fri, 30 Jun 2017 17:56:19 GMT'}, {'version': 'v2', 'created': 'Thu, 15 Feb 2018 16:00:54 GMT'}, {'version': 'v3', 'created': 'Tue, 9 Jul 2019 09:57:23 GMT'}]",2019-11-01,"[['Fortunato', 'Meire', ''], ['Azar', 'Mohammad Gheshlaghi', ''], ['Piot', 'Bilal', ''], ['Menick', 'Jacob', ''], ['Osband', 'Ian', ''], ['Graves', 'Alex', ''], ['Mnih', 'Vlad', ''], ['Munos', 'Remi', ''], ['Hassabis', 'Demis', ''], ['Pietquin', 'Olivier', ''], ['Blundell', 'Charles', ''], ['Legg', 'Shane', '']]",2019,11,1,Noisy Networks for Exploration,69,zscore: 3.561970,"We introduce NoisyNet, a deep reinforcement learning agent with parametric noise added to its weights, and show that the induced stochasticity of the agent's policy can be used to aid efficient exploration. The parameters of the noise are learned with gradient descent along with the remaining network weights. NoisyNet is straightforward to implement and adds little computational overhead. We find that replacing the conventional exploration heuristics for A3C, DQN and dueling agents (entropy reward and $\epsilon$-greedy respectively) with NoisyNet yields substantially higher scores for a wide range of Atari games, in some cases advancing the agent from sub to super-human performance.",Games,,Reinforcement Learning,,Better accuracy,,,
16,1707.01495,Marcin Andrychowicz,"Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel
  Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel, Wojciech Zaremba",Hindsight Experience Replay,,,,,cs.LG cs.AI cs.NE cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Dealing with sparse rewards is one of the biggest challenges in Reinforcement
Learning (RL). We present a novel technique called Hindsight Experience Replay
which allows sample-efficient learning from rewards which are sparse and binary
and therefore avoid the need for complicated reward engineering. It can be
combined with an arbitrary off-policy RL algorithm and may be seen as a form of
implicit curriculum.
  We demonstrate our approach on the task of manipulating objects with a
robotic arm. In particular, we run experiments on three different tasks:
pushing, sliding, and pick-and-place, in each case using only binary rewards
indicating whether or not the task is completed. Our ablation studies show that
Hindsight Experience Replay is a crucial ingredient which makes training
possible in these challenging environments. We show that our policies trained
on a physics simulation can be deployed on a physical robot and successfully
complete the task.
","[{'version': 'v1', 'created': 'Wed, 5 Jul 2017 17:55:53 GMT'}, {'version': 'v2', 'created': 'Mon, 10 Jul 2017 18:35:33 GMT'}, {'version': 'v3', 'created': 'Fri, 23 Feb 2018 10:04:20 GMT'}]",2018-02-26,"[['Andrychowicz', 'Marcin', ''], ['Wolski', 'Filip', ''], ['Ray', 'Alex', ''], ['Schneider', 'Jonas', ''], ['Fong', 'Rachel', ''], ['Welinder', 'Peter', ''], ['McGrew', 'Bob', ''], ['Tobin', 'Josh', ''], ['Abbeel', 'Pieter', ''], ['Zaremba', 'Wojciech', '']]",2018,2,26,Hindsight Experience Replay,97,zscore: 5.500657,"Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum.   We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task.",Locomotion,,Reinforcement Learning,,Better accuracy,,,
17,1707.06347,John Schulman,"John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg
  Klimov",Proximal Policy Optimization Algorithms,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a ""surrogate"" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time.
","[{'version': 'v1', 'created': 'Thu, 20 Jul 2017 02:32:33 GMT'}, {'version': 'v2', 'created': 'Mon, 28 Aug 2017 09:20:06 GMT'}]",2017-08-29,"[['Schulman', 'John', ''], ['Wolski', 'Filip', ''], ['Dhariwal', 'Prafulla', ''], ['Radford', 'Alec', ''], ['Klimov', 'Oleg', '']]",2017,8,29,Proximal Policy Optimization Algorithms,389,zscore: 12.533504,"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ""surrogate"" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",Various,,Reinforcement Learning,,Better accuracy,,,
18,1707.07012,Quoc Le,"Barret Zoph, Vijay Vasudevan, Jonathon Shlens, Quoc V. Le",Learning Transferable Architectures for Scalable Image Recognition,,,,,cs.CV cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Developing neural network image classification models often requires
significant architecture engineering. In this paper, we study a method to learn
the model architectures directly on the dataset of interest. As this approach
is expensive when the dataset is large, we propose to search for an
architectural building block on a small dataset and then transfer the block to
a larger dataset. The key contribution of this work is the design of a new
search space (the ""NASNet search space"") which enables transferability. In our
experiments, we search for the best convolutional layer (or ""cell"") on the
CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
together more copies of this cell, each with their own parameters to design a
convolutional architecture, named ""NASNet architecture"". We also introduce a
new regularization technique called ScheduledDropPath that significantly
improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
the best human-invented architectures while having 9 billion fewer FLOPS - a
reduction of 28% in computational demand from the previous state-of-the-art
model. When evaluated at different levels of computational cost, accuracies of
NASNets exceed those of the state-of-the-art human-designed models. For
instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
3.1% better than equivalently-sized, state-of-the-art models for mobile
platforms. Finally, the learned features by NASNet used with the Faster-RCNN
framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
dataset.
","[{'version': 'v1', 'created': 'Fri, 21 Jul 2017 18:10:26 GMT'}, {'version': 'v2', 'created': 'Wed, 25 Oct 2017 01:37:56 GMT'}, {'version': 'v3', 'created': 'Fri, 1 Dec 2017 07:48:01 GMT'}, {'version': 'v4', 'created': 'Wed, 11 Apr 2018 05:12:21 GMT'}]",2018-04-12,"[['Zoph', 'Barret', ''], ['Vasudevan', 'Vijay', ''], ['Shlens', 'Jonathon', ''], ['Le', 'Quoc V.', '']]",2018,4,12,Learning Transferable Architectures for Scalable Image Recognition,196,zscore: 6.047595,"Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the ""NASNet search space"") which enables transferability. In our experiments, we search for the best convolutional layer (or ""cell"") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named ""NASNet architecture"". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.",Image ,,Architecture Search,,Better accuracy,,,
19,1707.07328,Robin Jia,Robin Jia and Percy Liang,Adversarial Examples for Evaluating Reading Comprehension Systems,EMNLP 2017,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Standard accuracy metrics indicate that reading comprehension systems are
making rapid progress, but the extent to which these systems truly understand
language remains unclear. To reward systems with real language understanding
abilities, we propose an adversarial evaluation scheme for the Stanford
Question Answering Dataset (SQuAD). Our method tests whether systems can answer
questions about paragraphs that contain adversarially inserted sentences, which
are automatically generated to distract computer systems without changing the
correct answer or misleading humans. In this adversarial setting, the accuracy
of sixteen published models drops from an average of $75\%$ F1 score to $36\%$;
when the adversary is allowed to add ungrammatical sequences of words, average
accuracy on four models decreases further to $7\%$. We hope our insights will
motivate the development of new models that understand language more precisely.
","[{'version': 'v1', 'created': 'Sun, 23 Jul 2017 18:26:29 GMT'}]",2017-07-25,"[['Jia', 'Robin', ''], ['Liang', 'Percy', '']]",2017,7,25,Adversarial Examples for Evaluating Reading Comprehension Systems,135,zscore: 3.976484,"Standard accuracy metrics indicate that reading comprehension systems are making rapid progress, but the extent to which these systems truly understand language remains unclear. To reward systems with real language understanding abilities, we propose an adversarial evaluation scheme for the Stanford Question Answering Dataset (SQuAD). Our method tests whether systems can answer questions about paragraphs that contain adversarially inserted sentences, which are automatically generated to distract computer systems without changing the correct answer or misleading humans. In this adversarial setting, the accuracy of sixteen published models drops from an average of $75\%$ F1 score to $36\%$; when the adversary is allowed to add ungrammatical sequences of words, average accuracy on four models decreases further to $7\%$. We hope our insights will motivate the development of new models that understand language more precisely.",Text,QA,Adversarial,Examples,Exposing Weaknesses,,,
20,1707.09405,Qifeng Chen,Qifeng Chen and Vladlen Koltun,Photographic Image Synthesis with Cascaded Refinement Networks,"Published at the International Conference on Computer Vision (ICCV
  2017)",,,,cs.CV cs.AI cs.GR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present an approach to synthesizing photographic images conditioned on
semantic layouts. Given a semantic label map, our approach produces an image
with photographic appearance that conforms to the input layout. The approach
thus functions as a rendering engine that takes a two-dimensional semantic
specification of the scene and produces a corresponding photographic image.
Unlike recent and contemporaneous work, our approach does not rely on
adversarial training. We show that photographic images can be synthesized from
semantic layouts by a single feedforward network with appropriate structure,
trained end-to-end with a direct regression objective. The presented approach
scales seamlessly to high resolutions; we demonstrate this by synthesizing
photographic images at 2-megapixel resolution, the full resolution of our
training data. Extensive perceptual experiments on datasets of outdoor and
indoor scenes demonstrate that images synthesized by the presented approach are
considerably more realistic than alternative approaches. The results are shown
in the supplementary video at https://youtu.be/0fhUJT21-bs
","[{'version': 'v1', 'created': 'Fri, 28 Jul 2017 20:24:44 GMT'}]",2017-08-01,"[['Chen', 'Qifeng', ''], ['Koltun', 'Vladlen', '']]",2017,8,1,Photographic Image Synthesis with Cascaded Refinement Networks,144,zscore: 4.105115,"We present an approach to synthesizing photographic images conditioned on semantic layouts. Given a semantic label map, our approach produces an image with photographic appearance that conforms to the input layout. The approach thus functions as a rendering engine that takes a two-dimensional semantic specification of the scene and produces a corresponding photographic image. Unlike recent and contemporaneous work, our approach does not rely on adversarial training. We show that photographic images can be synthesized from semantic layouts by a single feedforward network with appropriate structure, trained end-to-end with a direct regression objective. The presented approach scales seamlessly to high resolutions; we demonstrate this by synthesizing photographic images at 2-megapixel resolution, the full resolution of our training data. Extensive perceptual experiments on datasets of outdoor and indoor scenes demonstrate that images synthesized by the presented approach are considerably more realistic than alternative approaches. The results are shown in the supplementary video at https://youtu.be/0fhUJT21-bs",Image ,,Deep Learning,Architectures (FFN),Better accuracy,,,
21,1708.00107,Bryan McCann,"Bryan McCann, James Bradbury, Caiming Xiong and Richard Socher",Learned in Translation: Contextualized Word Vectors,,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Computer vision has benefited from initializing multiple deep layers with
weights pretrained on large supervised training sets like ImageNet. Natural
language processing (NLP) typically sees initialization of only the lowest
layer of deep models with pretrained word vectors. In this paper, we use a deep
LSTM encoder from an attentional sequence-to-sequence model trained for machine
translation (MT) to contextualize word vectors. We show that adding these
context vectors (CoVe) improves performance over using only unsupervised word
and character vectors on a wide variety of common NLP tasks: sentiment analysis
(SST, IMDb), question classification (TREC), entailment (SNLI), and question
answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe
improves performance of our baseline models to the state of the art.
","[{'version': 'v1', 'created': 'Tue, 1 Aug 2017 00:05:34 GMT'}, {'version': 'v2', 'created': 'Wed, 20 Jun 2018 13:15:06 GMT'}]",2018-06-21,"[['McCann', 'Bryan', ''], ['Bradbury', 'James', ''], ['Xiong', 'Caiming', ''], ['Socher', 'Richard', '']]",2018,6,21,Learned in Translation: Contextualized Word Vectors,114,zscore: 4.605456,"Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",Text,,Representation Learning,,Better accuracy,,,
22,1708.02182,Stephen Merity,"Stephen Merity, Nitish Shirish Keskar, Richard Socher",Regularizing and Optimizing LSTM Language Models,,,,,cs.CL cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recurrent neural networks (RNNs), such as long short-term memory networks
(LSTMs), serve as a fundamental building block for many sequence learning
tasks, including machine translation, language modeling, and question
answering. In this paper, we consider the specific problem of word-level
language modeling and investigate strategies for regularizing and optimizing
LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on
hidden-to-hidden weights as a form of recurrent regularization. Further, we
introduce NT-ASGD, a variant of the averaged stochastic gradient method,
wherein the averaging trigger is determined using a non-monotonic condition as
opposed to being tuned by the user. Using these and other regularization
strategies, we achieve state-of-the-art word level perplexities on two data
sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the
effectiveness of a neural cache in conjunction with our proposed model, we
achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and
52.0 on WikiText-2.
","[{'version': 'v1', 'created': 'Mon, 7 Aug 2017 16:03:44 GMT'}]",2017-08-09,"[['Merity', 'Stephen', ''], ['Keskar', 'Nitish Shirish', ''], ['Socher', 'Richard', '']]",2017,8,9,Regularizing and Optimizing LSTM Language Models,119,zscore: 5.829660,"Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.",Text,,Deep Learning,Architectures (RNN),Better accuracy,,,
23,1708.03999,Huan Zhang,"Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh","ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural
  Networks without Training Substitute Models","Accepted by 10th ACM Workshop on Artificial Intelligence and Security
  (AISEC) with the 24th ACM Conference on Computer and Communications Security
  (CCS)",,10.1145/3128572.3140448,,stat.ML cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks (DNNs) are one of the most prominent technologies of our
time, as they achieve state-of-the-art performance in many machine learning
tasks, including but not limited to image classification, text mining, and
speech processing. However, recent research on DNNs has indicated
ever-increasing concern on the robustness to adversarial examples, especially
for security-critical tasks such as traffic sign identification for autonomous
driving. Studies have unveiled the vulnerability of a well-trained DNN by
demonstrating the ability of generating barely noticeable (to both human and
machines) adversarial images that lead to misclassification. Furthermore,
researchers have shown that these adversarial images are highly transferable by
simply training and attacking a substitute model built upon the target model,
known as a black-box attack to DNNs.
  Similar to the setting of training substitute models, in this paper we
propose an effective black-box attack that also only has access to the input
(images) and the output (confidence scores) of a targeted DNN. However,
different from leveraging attack transferability from substitute models, we
propose zeroth order optimization (ZOO) based attacks to directly estimate the
gradients of the targeted DNN for generating adversarial examples. We use
zeroth order stochastic coordinate descent along with dimension reduction,
hierarchical attack and importance sampling techniques to efficiently attack
black-box models. By exploiting zeroth order optimization, improved attacks to
the targeted DNN can be accomplished, sparing the need for training substitute
models and avoiding the loss in attack transferability. Experimental results on
MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective
as the state-of-the-art white-box attack and significantly outperforms existing
black-box attacks via substitute models.
","[{'version': 'v1', 'created': 'Mon, 14 Aug 2017 03:48:03 GMT'}, {'version': 'v2', 'created': 'Thu, 2 Nov 2017 04:18:44 GMT'}]",2017-11-03,"[['Chen', 'Pin-Yu', ''], ['Zhang', 'Huan', ''], ['Sharma', 'Yash', ''], ['Yi', 'Jinfeng', ''], ['Hsieh', 'Cho-Jui', '']]",2017,11,3,ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural   Networks without Training Substitute Models,85,zscore: 4.963248,"Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs.   Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack and significantly outperforms existing black-box attacks via substitute models.",Image ,,Adversarial,Attacks,Exposing Weaknesses,,,
24,1708.04485,Stephen Keckler,"Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli,
  Rangharajan Venkatesan, Brucek Khailany, Joel Emer, Stephen W. Keckler, and
  William J. Dally",SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks,,,,,cs.NE cs.AR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Convolutional Neural Networks (CNNs) have emerged as a fundamental technology
for machine learning. High performance and extreme energy efficiency are
critical for deployments of CNNs in a wide range of situations, especially
mobile platforms such as autonomous vehicles, cameras, and electronic personal
assistants. This paper introduces the Sparse CNN (SCNN) accelerator
architecture, which improves performance and energy efficiency by exploiting
the zero-valued weights that stem from network pruning during training and
zero-valued activations that arise from the common ReLU operator applied during
inference. Specifically, SCNN employs a novel dataflow that enables maintaining
the sparse weights and activations in a compressed encoding, which eliminates
unnecessary data transfers and reduces storage requirements. Furthermore, the
SCNN dataflow facilitates efficient delivery of those weights and activations
to the multiplier array, where they are extensively reused. In addition, the
accumulation of multiplication products are performed in a novel accumulator
array. Our results show that on contemporary neural networks, SCNN can improve
both performance and energy by a factor of 2.7x and 2.3x, respectively, over a
comparably provisioned dense CNN accelerator.
","[{'version': 'v1', 'created': 'Tue, 23 May 2017 22:11:11 GMT'}]",2017-08-16,"[['Parashar', 'Angshuman', ''], ['Rhu', 'Minsoo', ''], ['Mukkara', 'Anurag', ''], ['Puglielli', 'Antonio', ''], ['Venkatesan', 'Rangharajan', ''], ['Khailany', 'Brucek', ''], ['Emer', 'Joel', ''], ['Keckler', 'Stephen W.', ''], ['Dally', 'William J.', '']]",2017,8,16,SCNN: An Accelerator for Compressed-sparse Convolutional Neural Networks,110,zscore: 3.896850,"Convolutional Neural Networks (CNNs) have emerged as a fundamental technology for machine learning. High performance and extreme energy efficiency are critical for deployments of CNNs in a wide range of situations, especially mobile platforms such as autonomous vehicles, cameras, and electronic personal assistants. This paper introduces the Sparse CNN (SCNN) accelerator architecture, which improves performance and energy efficiency by exploiting the zero-valued weights that stem from network pruning during training and zero-valued activations that arise from the common ReLU operator applied during inference. Specifically, SCNN employs a novel dataflow that enables maintaining the sparse weights and activations in a compressed encoding, which eliminates unnecessary data transfers and reduces storage requirements. Furthermore, the SCNN dataflow facilitates efficient delivery of those weights and activations to the multiplier array, where they are extensively reused. In addition, the accumulation of multiplication products are performed in a novel accumulator array. Our results show that on contemporary neural networks, SCNN can improve both performance and energy by a factor of 2.7x and 2.3x, respectively, over a comparably provisioned dense CNN accelerator.",Various,,Deep Learning,Architectures (CNN),Faster,,,
25,1708.05144,Yuhuai(Tony) Wu,"Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, Jimmy Ba","Scalable trust-region method for deep reinforcement learning using
  Kronecker-factored approximation","14 pages, 9 figures; update github repo link",,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we propose to apply trust region optimization to deep
reinforcement learning using a recently proposed Kronecker-factored
approximation to the curvature. We extend the framework of natural policy
gradient and propose to optimize both the actor and the critic using
Kronecker-factored approximate curvature (K-FAC) with trust region; hence we
call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To
the best of our knowledge, this is the first scalable trust region natural
gradient method for actor-critic methods. It is also a method that learns
non-trivial tasks in continuous control as well as discrete control policies
directly from raw pixel inputs. We tested our approach across discrete domains
in Atari games as well as continuous domains in the MuJoCo environment. With
the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold
improvement in sample efficiency on average, compared to previous
state-of-the-art on-policy actor-critic methods. Code is available at
https://github.com/openai/baselines
","[{'version': 'v1', 'created': 'Thu, 17 Aug 2017 06:14:48 GMT'}, {'version': 'v2', 'created': 'Fri, 18 Aug 2017 11:16:36 GMT'}]",2017-08-21,"[['Wu', 'Yuhuai', ''], ['Mansimov', 'Elman', ''], ['Liao', 'Shun', ''], ['Grosse', 'Roger', ''], ['Ba', 'Jimmy', '']]",2017,8,21,Scalable trust-region method for deep reinforcement learning using   Kronecker-factored approximation,68,zscore: 3.780912,"In this work, we propose to apply trust region optimization to deep reinforcement learning using a recently proposed Kronecker-factored approximation to the curvature. We extend the framework of natural policy gradient and propose to optimize both the actor and the critic using Kronecker-factored approximate curvature (K-FAC) with trust region; hence we call our method Actor Critic using Kronecker-Factored Trust Region (ACKTR). To the best of our knowledge, this is the first scalable trust region natural gradient method for actor-critic methods. It is also a method that learns non-trivial tasks in continuous control as well as discrete control policies directly from raw pixel inputs. We tested our approach across discrete domains in Atari games as well as continuous domains in the MuJoCo environment. With the proposed methods, we are able to achieve higher rewards and a 2- to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic methods. Code is available at https://github.com/openai/baselines",Games,,Reinforcement Learning,,Better accuracy,,,
26,1708.05866,Kai Arulkumaran,"Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony
  Bharath",A Brief Survey of Deep Reinforcement Learning,"IEEE Signal Processing Magazine, Special Issue on Deep Learning for
  Image Understanding (arXiv extended version)",,10.1109/MSP.2017.2743240,,cs.LG cs.AI cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep reinforcement learning is poised to revolutionise the field of AI and
represents a step towards building autonomous systems with a higher level
understanding of the visual world. Currently, deep learning is enabling
reinforcement learning to scale to problems that were previously intractable,
such as learning to play video games directly from pixels. Deep reinforcement
learning algorithms are also applied to robotics, allowing control policies for
robots to be learned directly from camera inputs in the real world. In this
survey, we begin with an introduction to the general field of reinforcement
learning, then progress to the main streams of value-based and policy-based
methods. Our survey will cover central algorithms in deep reinforcement
learning, including the deep $Q$-network, trust region policy optimisation, and
asynchronous advantage actor-critic. In parallel, we highlight the unique
advantages of deep neural networks, focusing on visual understanding via
reinforcement learning. To conclude, we describe several current areas of
research within the field.
","[{'version': 'v1', 'created': 'Sat, 19 Aug 2017 15:55:31 GMT'}, {'version': 'v2', 'created': 'Thu, 28 Sep 2017 21:51:43 GMT'}]",2017-11-15,"[['Arulkumaran', 'Kai', ''], ['Deisenroth', 'Marc Peter', ''], ['Brundage', 'Miles', ''], ['Bharath', 'Anil Anthony', '']]",2017,11,15,A Brief Survey of Deep Reinforcement Learning,60,zscore: 3.741702,"Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep $Q$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.",General,,Reinforcement Learning,,Review,,,
27,1708.06519,Zhuang Liu,"Zhuang Liu and Jianguo Li and Zhiqiang Shen and Gao Huang and Shoumeng
  Yan and Changshui Zhang",Learning Efficient Convolutional Networks through Network Slimming,Accepted by ICCV 2017,,,,cs.CV cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The deployment of deep convolutional neural networks (CNNs) in many real
world applications is largely hindered by their high computational cost. In
this paper, we propose a novel learning scheme for CNNs to simultaneously 1)
reduce the model size; 2) decrease the run-time memory footprint; and 3) lower
the number of computing operations, without compromising accuracy. This is
achieved by enforcing channel-level sparsity in the network in a simple but
effective way. Different from many existing approaches, the proposed method
directly applies to modern CNN architectures, introduces minimum overhead to
the training process, and requires no special software/hardware accelerators
for the resulting models. We call our approach network slimming, which takes
wide and large networks as input models, but during training insignificant
channels are automatically identified and pruned afterwards, yielding thin and
compact models with comparable accuracy. We empirically demonstrate the
effectiveness of our approach with several state-of-the-art CNN models,
including VGGNet, ResNet and DenseNet, on various image classification
datasets. For VGGNet, a multi-pass version of network slimming gives a 20x
reduction in model size and a 5x reduction in computing operations.
","[{'version': 'v1', 'created': 'Tue, 22 Aug 2017 07:35:26 GMT'}]",2017-08-23,"[['Liu', 'Zhuang', ''], ['Li', 'Jianguo', ''], ['Shen', 'Zhiqiang', ''], ['Huang', 'Gao', ''], ['Yan', 'Shoumeng', ''], ['Zhang', 'Changshui', '']]",2017,8,23,Learning Efficient Convolutional Networks through Network Slimming,82,zscore: 5.232111,"The deployment of deep convolutional neural networks (CNNs) in many real world applications is largely hindered by their high computational cost. In this paper, we propose a novel learning scheme for CNNs to simultaneously 1) reduce the model size; 2) decrease the run-time memory footprint; and 3) lower the number of computing operations, without compromising accuracy. This is achieved by enforcing channel-level sparsity in the network in a simple but effective way. Different from many existing approaches, the proposed method directly applies to modern CNN architectures, introduces minimum overhead to the training process, and requires no special software/hardware accelerators for the resulting models. We call our approach network slimming, which takes wide and large networks as input models, but during training insignificant channels are automatically identified and pruned afterwards, yielding thin and compact models with comparable accuracy. We empirically demonstrate the effectiveness of our approach with several state-of-the-art CNN models, including VGGNet, ResNet and DenseNet, on various image classification datasets. For VGGNet, a multi-pass version of network slimming gives a 20x reduction in model size and a 5x reduction in computing operations.",Image ,,Deep Learning,Architectures (CNN),Faster,,,
28,1708.07524,Jitong Chen,DeLiang Wang and Jitong Chen,Supervised Speech Separation Based on Deep Learning: An Overview,"27 pages, 17 figures",,,,cs.CL cs.LG cs.NE cs.SD,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Speech separation is the task of separating target speech from background
interference. Traditionally, speech separation is studied as a signal
processing problem. A more recent approach formulates speech separation as a
supervised learning problem, where the discriminative patterns of speech,
speakers, and background noise are learned from training data. Over the past
decade, many supervised separation algorithms have been put forward. In
particular, the recent introduction of deep learning to supervised speech
separation has dramatically accelerated progress and boosted separation
performance. This article provides a comprehensive overview of the research on
deep learning based supervised speech separation in the last several years. We
first introduce the background of speech separation and the formulation of
supervised separation. Then we discuss three main components of supervised
separation: learning machines, training targets, and acoustic features. Much of
the overview is on separation algorithms where we review monaural methods,
including speech enhancement (speech-nonspeech separation), speaker separation
(multi-talker separation), and speech dereverberation, as well as
multi-microphone techniques. The important issue of generalization, unique to
supervised learning, is discussed. This overview provides a historical
perspective on how advances are made. In addition, we discuss a number of
conceptual issues, including what constitutes the target source.
","[{'version': 'v1', 'created': 'Thu, 24 Aug 2017 18:51:50 GMT'}, {'version': 'v2', 'created': 'Fri, 15 Jun 2018 03:28:26 GMT'}]",2018-06-18,"[['Wang', 'DeLiang', ''], ['Chen', 'Jitong', '']]",2018,6,18,Supervised Speech Separation Based on Deep Learning: An Overview,61,zscore: 4.155743,"Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This article provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multi-talker separation), and speech dereverberation, as well as multi-microphone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.",Speech,,Various,,Review,,,
29,1708.08559,Baishakhi Ray,"Yuchi Tian, Kexin Pei, Suman Jana, Baishakhi Ray","DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous
  Cars",,,,,cs.SE cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent advances in Deep Neural Networks (DNNs) have led to the development of
DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can
drive without any human intervention. Most major manufacturers including Tesla,
GM, Ford, BMW, and Waymo/Google are working on building and testing different
types of autonomous vehicles. The lawmakers of several US states including
California, Texas, and New York have passed new legislation to fast-track the
process of testing and deployment of autonomous vehicles on their roads.
  However, despite their spectacular progress, DNNs, just like traditional
software, often demonstrate incorrect or unexpected corner case behaviors that
can lead to potentially fatal collisions. Several such real-world accidents
involving autonomous cars have already happened including one which resulted in
a fatality. Most existing testing techniques for DNN-driven vehicles are
heavily dependent on the manual collection of test data under different driving
conditions which become prohibitively expensive as the number of test
conditions increases.
  In this paper, we design, implement and evaluate DeepTest, a systematic
testing tool for automatically detecting erroneous behaviors of DNN-driven
vehicles that can potentially lead to fatal crashes. First, our tool is
designed to automatically generated test cases leveraging real-world changes in
driving conditions like rain, fog, lighting conditions, etc. DeepTest
systematically explores different parts of the DNN logic by generating test
inputs that maximize the numbers of activated neurons. DeepTest found thousands
of erroneous behaviors under different realistic driving conditions (e.g.,
blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in
three top performing DNNs in the Udacity self-driving car challenge.
","[{'version': 'v1', 'created': 'Mon, 28 Aug 2017 23:26:14 GMT'}, {'version': 'v2', 'created': 'Tue, 20 Mar 2018 06:10:24 GMT'}]",2018-03-21,"[['Tian', 'Yuchi', ''], ['Pei', 'Kexin', ''], ['Jana', 'Suman', ''], ['Ray', 'Baishakhi', '']]",2018,3,21,DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous   Cars,56,zscore: 4.334828,"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads.   However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases.   In this paper, we design, implement and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explores different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.",Autonomous driving,,None,,Exposing Weaknesses,,,
30,1709.01215,Chunyuan Li,"Chunyuan Li, Hao Liu, Changyou Chen, Yunchen Pu, Liqun Chen, Ricardo
  Henao, Lawrence Carin","ALICE: Towards Understanding Adversarial Learning for Joint Distribution
  Matching","NIPS 2017 (22 pages); short version (9 pages):
  http://people.duke.edu/~cl319/doc/papers/nips_2017_alice.pdf",,,,stat.ML cs.AI cs.CV cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We investigate the non-identifiability issues associated with bidirectional
adversarial training for joint distribution matching. Within a framework of
conditional entropy, we propose both adversarial and non-adversarial approaches
to learn desirable matched joint distributions for unsupervised and supervised
tasks. We unify a broad family of adversarial models as joint distribution
matching problems. Our approach stabilizes learning of unsupervised
bidirectional adversarial learning methods. Further, we introduce an extension
for semi-supervised learning tasks. Theoretical results are validated in
synthetic data and real-world applications.
","[{'version': 'v1', 'created': 'Tue, 5 Sep 2017 02:18:06 GMT'}, {'version': 'v2', 'created': 'Sun, 5 Nov 2017 03:58:52 GMT'}]",2017-11-07,"[['Li', 'Chunyuan', ''], ['Liu', 'Hao', ''], ['Chen', 'Changyou', ''], ['Pu', 'Yunchen', ''], ['Chen', 'Liqun', ''], ['Henao', 'Ricardo', ''], ['Carin', 'Lawrence', '']]",2017,11,7,ALICE: Towards Understanding Adversarial Learning for Joint Distribution   Matching,42,zscore: 3.999466,"We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.",Theoretical,,Adversarial,Training,Generalization,,,
31,1709.04114,Pin-Yu Chen,"Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi and Cho-Jui Hsieh","EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial
  Examples",To be published at AAAI 2018,,,,stat.ML cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent studies have highlighted the vulnerability of deep neural networks
(DNNs) to adversarial examples - a visually indistinguishable adversarial image
can easily be crafted to cause a well-trained model to misclassify. Existing
methods for crafting adversarial examples are based on $L_2$ and $L_\infty$
distortion metrics. However, despite the fact that $L_1$ distortion accounts
for the total variation and encourages sparsity in the perturbation, little has
been developed for crafting $L_1$-based adversarial examples. In this paper, we
formulate the process of attacking DNNs via adversarial examples as an
elastic-net regularized optimization problem. Our elastic-net attacks to DNNs
(EAD) feature $L_1$-oriented adversarial examples and include the
state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST,
CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial
examples with small $L_1$ distortion and attains similar attack performance to
the state-of-the-art methods in different attack scenarios. More importantly,
EAD leads to improved attack transferability and complements adversarial
training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in
adversarial machine learning and security implications of DNNs.
","[{'version': 'v1', 'created': 'Wed, 13 Sep 2017 02:40:59 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Nov 2017 19:59:47 GMT'}, {'version': 'v3', 'created': 'Sat, 10 Feb 2018 04:49:12 GMT'}]",2018-02-13,"[['Chen', 'Pin-Yu', ''], ['Sharma', 'Yash', ''], ['Zhang', 'Huan', ''], ['Yi', 'Jinfeng', ''], ['Hsieh', 'Cho-Jui', '']]",2018,2,13,EAD: Elastic-Net Attacks to Deep Neural Networks via Adversarial   Examples,61,zscore: 4.627727,"Recent studies have highlighted the vulnerability of deep neural networks (DNNs) to adversarial examples - a visually indistinguishable adversarial image can easily be crafted to cause a well-trained model to misclassify. Existing methods for crafting adversarial examples are based on $L_2$ and $L_\infty$ distortion metrics. However, despite the fact that $L_1$ distortion accounts for the total variation and encourages sparsity in the perturbation, little has been developed for crafting $L_1$-based adversarial examples. In this paper, we formulate the process of attacking DNNs via adversarial examples as an elastic-net regularized optimization problem. Our elastic-net attacks to DNNs (EAD) feature $L_1$-oriented adversarial examples and include the state-of-the-art $L_2$ attack as a special case. Experimental results on MNIST, CIFAR10 and ImageNet show that EAD can yield a distinct set of adversarial examples with small $L_1$ distortion and attains similar attack performance to the state-of-the-art methods in different attack scenarios. More importantly, EAD leads to improved attack transferability and complements adversarial training for DNNs, suggesting novel insights on leveraging $L_1$ distortion in adversarial machine learning and security implications of DNNs.",Image ,,Adversarial,Attacks,Exposing Weaknesses,,,
32,1709.05584,William L Hamilton,"William L. Hamilton, Rex Ying, and Jure Leskovec",Representation Learning on Graphs: Methods and Applications,"Published in the IEEE Data Engineering Bulletin, September 2017;
  version with minor corrections",,,,cs.SI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Machine learning on graphs is an important and ubiquitous task with
applications ranging from drug design to friendship recommendation in social
networks. The primary challenge in this domain is finding a way to represent,
or encode, graph structure so that it can be easily exploited by machine
learning models. Traditionally, machine learning approaches relied on
user-defined heuristics to extract features encoding structural information
about a graph (e.g., degree statistics or kernel functions). However, recent
years have seen a surge in approaches that automatically learn to encode graph
structure into low-dimensional embeddings, using techniques based on deep
learning and nonlinear dimensionality reduction. Here we provide a conceptual
review of key advancements in this area of representation learning on graphs,
including matrix factorization-based methods, random-walk based algorithms, and
graph neural networks. We review methods to embed individual nodes as well as
approaches to embed entire (sub)graphs. In doing so, we develop a unified
framework to describe these recent approaches, and we highlight a number of
important applications and directions for future work.
","[{'version': 'v1', 'created': 'Sun, 17 Sep 2017 00:19:33 GMT'}, {'version': 'v2', 'created': 'Wed, 27 Sep 2017 22:05:19 GMT'}, {'version': 'v3', 'created': 'Tue, 10 Apr 2018 15:26:32 GMT'}]",2018-04-11,"[['Hamilton', 'William L.', ''], ['Ying', 'Rex', ''], ['Leskovec', 'Jure', '']]",2018,4,11,Representation Learning on Graphs: Methods and Applications,99,zscore: 7.846062,"Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.",Graphs,,Representation Learning,,Review,,,
33,1709.06560,Peter Henderson,"Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina
  Precup, David Meger",Deep Reinforcement Learning that Matters,"Accepted to the Thirthy-Second AAAI Conference On Artificial
  Intelligence (AAAI), 2018",,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In recent years, significant progress has been made in solving challenging
problems across various domains using deep reinforcement learning (RL).
Reproducing existing work and accurately judging the improvements offered by
novel methods is vital to sustaining this progress. Unfortunately, reproducing
results for state-of-the-art deep RL methods is seldom straightforward. In
particular, non-determinism in standard benchmark environments, combined with
variance intrinsic to the methods, can make reported results tough to
interpret. Without significance metrics and tighter standardization of
experimental reporting, it is difficult to determine whether improvements over
the prior state-of-the-art are meaningful. In this paper, we investigate
challenges posed by reproducibility, proper experimental techniques, and
reporting procedures. We illustrate the variability in reported metrics and
results when comparing against common baselines and suggest guidelines to make
future results in deep RL more reproducible. We aim to spur discussion about
how to ensure continued progress in the field by minimizing wasted effort
stemming from results that are non-reproducible and easily misinterpreted.
","[{'version': 'v1', 'created': 'Tue, 19 Sep 2017 06:09:47 GMT'}, {'version': 'v2', 'created': 'Fri, 24 Nov 2017 19:51:33 GMT'}, {'version': 'v3', 'created': 'Wed, 30 Jan 2019 04:21:41 GMT'}]",2019-01-31,"[['Henderson', 'Peter', ''], ['Islam', 'Riashat', ''], ['Bachman', 'Philip', ''], ['Pineau', 'Joelle', ''], ['Precup', 'Doina', ''], ['Meger', 'David', '']]",2019,1,31,Deep Reinforcement Learning that Matters,118,zscore: 9.394803,"In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.",Various,,Reinforcement Learning,,Exposing Weaknesses,of researchers,,
34,1710.02298,Matteo Hessel,"Matteo Hessel, Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg
  Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver",Rainbow: Combining Improvements in Deep Reinforcement Learning,Under review as a conference paper at AAAI 2018,,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The deep reinforcement learning community has made several independent
improvements to the DQN algorithm. However, it is unclear which of these
extensions are complementary and can be fruitfully combined. This paper
examines six extensions to the DQN algorithm and empirically studies their
combination. Our experiments show that the combination provides
state-of-the-art performance on the Atari 2600 benchmark, both in terms of data
efficiency and final performance. We also provide results from a detailed
ablation study that shows the contribution of each component to overall
performance.
","[{'version': 'v1', 'created': 'Fri, 6 Oct 2017 07:45:46 GMT'}]",2017-10-09,"[['Hessel', 'Matteo', ''], ['Modayil', 'Joseph', ''], ['van Hasselt', 'Hado', ''], ['Schaul', 'Tom', ''], ['Ostrovski', 'Georg', ''], ['Dabney', 'Will', ''], ['Horgan', 'Dan', ''], ['Piot', 'Bilal', ''], ['Azar', 'Mohammad', ''], ['Silver', 'David', '']]",2017,10,9,Rainbow: Combining Improvements in Deep Reinforcement Learning,79,zscore: 7.178528,"The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.",Games,,Reinforcement Learning,,Better accuracy,,,
35,1710.02971,Jiezhong Qiu,"Jiezhong Qiu, Yuxiao Dong, Hao Ma, Jian Li, Kuansan Wang, Jie Tang","Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,
  and node2vec","9 pages, published in WSDM 2018 proceedings",,10.1145/3159652.3159706,,cs.SI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Since the invention of word2vec, the skip-gram model has significantly
advanced the research of network embedding, such as the recent emergence of the
DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of
the aforementioned models with negative sampling can be unified into the matrix
factorization framework with closed forms. Our analysis and proofs reveal that:
(1) DeepWalk empirically produces a low-rank transformation of a network's
normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk
when the size of vertices' context is set to one; (3) As an extension of LINE,
PTE can be viewed as the joint factorization of multiple networks' Laplacians;
(4) node2vec is factorizing a matrix related to the stationary distribution and
transition probability tensor of a 2nd-order random walk. We further provide
the theoretical connections between skip-gram based network embedding
algorithms and the theory of graph Laplacian. Finally, we present the NetMF
method as well as its approximation algorithm for computing network embedding.
Our method offers significant improvements over DeepWalk and LINE for
conventional network mining tasks. This work lays the theoretical foundation
for skip-gram based network embedding methods, leading to a better
understanding of latent network representation learning.
","[{'version': 'v1', 'created': 'Mon, 9 Oct 2017 07:28:46 GMT'}, {'version': 'v2', 'created': 'Wed, 11 Oct 2017 02:38:00 GMT'}, {'version': 'v3', 'created': 'Tue, 12 Dec 2017 06:33:35 GMT'}, {'version': 'v4', 'created': 'Thu, 8 Feb 2018 09:51:03 GMT'}]",2018-02-09,"[['Qiu', 'Jiezhong', ''], ['Dong', 'Yuxiao', ''], ['Ma', 'Hao', ''], ['Li', 'Jian', ''], ['Wang', 'Kuansan', ''], ['Tang', 'Jie', '']]",2018,2,9,"Network Embedding as Matrix Factorization: Unifying DeepWalk, LINE, PTE,   and node2vec",52,zscore: 4.747513,"Since the invention of word2vec, the skip-gram model has significantly advanced the research of network embedding, such as the recent emergence of the DeepWalk, LINE, PTE, and node2vec approaches. In this work, we show that all of the aforementioned models with negative sampling can be unified into the matrix factorization framework with closed forms. Our analysis and proofs reveal that: (1) DeepWalk empirically produces a low-rank transformation of a network's normalized Laplacian matrix; (2) LINE, in theory, is a special case of DeepWalk when the size of vertices' context is set to one; (3) As an extension of LINE, PTE can be viewed as the joint factorization of multiple networks' Laplacians; (4) node2vec is factorizing a matrix related to the stationary distribution and transition probability tensor of a 2nd-order random walk. We further provide the theoretical connections between skip-gram based network embedding algorithms and the theory of graph Laplacian. Finally, we present the NetMF method as well as its approximation algorithm for computing network embedding. Our method offers significant improvements over DeepWalk and LINE for conventional network mining tasks. This work lays the theoretical foundation for skip-gram based network embedding methods, leading to a better understanding of latent network representation learning.",Graphs,,Representation Learning,,Better accuracy,,,
36,1710.05468,Kenji Kawaguchi,"Kenji Kawaguchi, Leslie Pack Kaelbling, Yoshua Bengio",Generalization in Deep Learning,"To appear in Mathematics of Deep Learning, Cambridge University
  Press. All previous results remain unchanged",,,"Massachusetts Institute of Technology (MIT), MIT-CSAIL-TR-2018-014",stat.ML cs.AI cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper provides theoretical insights into why and how deep learning can
generalize well, despite its large capacity, complexity, possible algorithmic
instability, nonrobustness, and sharp minima, responding to an open question in
the literature. We also discuss approaches to provide non-vacuous
generalization guarantees for deep learning. Based on theoretical observations,
we propose new open problems and discuss the limitations of our results.
","[{'version': 'v1', 'created': 'Mon, 16 Oct 2017 02:21:24 GMT'}, {'version': 'v2', 'created': 'Sun, 24 Dec 2017 19:44:43 GMT'}, {'version': 'v3', 'created': 'Thu, 22 Feb 2018 23:39:50 GMT'}, {'version': 'v4', 'created': 'Tue, 1 Jan 2019 00:07:45 GMT'}, {'version': 'v5', 'created': 'Fri, 10 May 2019 18:41:13 GMT'}, {'version': 'v6', 'created': 'Mon, 27 Jul 2020 23:01:04 GMT'}]",2020-07-29,"[['Kawaguchi', 'Kenji', ''], ['Kaelbling', 'Leslie Pack', ''], ['Bengio', 'Yoshua', '']]",2020,7,29,Generalization in Deep Learning,53,zscore: 4.089883,"With a direct analysis of neural networks, this paper presents a mathematically tight generalization theory to partially address an open problem regarding the generalization of deep learning. Unlike previous bound-based theory, our main theory is quantitatively as tight as possible for every dataset individually, while producing qualitative insights competitively. Our results give insight into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, answering to an open question in the literature. We also discuss limitations of our results and propose additional open problems.",Theoretical,,Analysis, ,Robustness,,,
37,1710.05941,Prajit Ramachandran,"Prajit Ramachandran, Barret Zoph, Quoc V. Le",Searching for Activation Functions,"Updated version of ""Swish: a Self-Gated Activation Function""",,,,cs.NE cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The choice of activation functions in deep networks has a significant effect
on the training dynamics and task performance. Currently, the most successful
and widely-used activation function is the Rectified Linear Unit (ReLU).
Although various hand-designed alternatives to ReLU have been proposed, none
have managed to replace it due to inconsistent gains. In this work, we propose
to leverage automatic search techniques to discover new activation functions.
Using a combination of exhaustive and reinforcement learning-based search, we
discover multiple novel activation functions. We verify the effectiveness of
the searches by conducting an empirical evaluation with the best discovered
activation function. Our experiments show that the best discovered activation
function, $f(x) = x \cdot \text{sigmoid}(\beta x)$, which we name Swish, tends
to work better than ReLU on deeper models across a number of challenging
datasets. For example, simply replacing ReLUs with Swish units improves top-1
classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for
Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it
easy for practitioners to replace ReLUs with Swish units in any neural network.
","[{'version': 'v1', 'created': 'Mon, 16 Oct 2017 18:05:45 GMT'}, {'version': 'v2', 'created': 'Fri, 27 Oct 2017 17:45:21 GMT'}]",2017-10-30,"[['Ramachandran', 'Prajit', ''], ['Zoph', 'Barret', ''], ['Le', 'Quoc V.', '']]",2017,10,30,Searching for Activation Functions,63,zscore: 4.963690,"The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, $f(x) = x \cdot \text{sigmoid}(\beta x)$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNet-A and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.",Various,,Architecture Search,,Evaluation,,,
38,1710.08864,Jiawei Su,"Jiawei Su, Danilo Vasconcellos Vargas and Sakurai Kouichi",One pixel attack for fooling deep neural networks,,"IEEE Transactions on Evolutionary Computation}, Vol.23 , Issue.5 ,
  pp. 828--841. Publisher: IEEE. 2019",10.1109/TEVC.2019.2890858,,cs.LG cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent research has revealed that the output of Deep Neural Networks (DNN)
can be easily altered by adding relatively small perturbations to the input
vector. In this paper, we analyze an attack in an extremely limited scenario
where only one pixel can be modified. For that we propose a novel method for
generating one-pixel adversarial perturbations based on differential evolution
(DE). It requires less adversarial information (a black-box attack) and can
fool more types of networks due to the inherent features of DE. The results
show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and
16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least
one target class by modifying just one pixel with 74.03% and 22.91% confidence
on average. We also show the same vulnerability on the original CIFAR-10
dataset. Thus, the proposed attack explores a different take on adversarial
machine learning in an extreme limited scenario, showing that current DNNs are
also vulnerable to such low dimension attacks. Besides, we also illustrate an
important application of DE (or broadly speaking, evolutionary computation) in
the domain of adversarial machine learning: creating tools that can effectively
generate low-cost adversarial attacks against neural networks for evaluating
robustness.
","[{'version': 'v1', 'created': 'Tue, 24 Oct 2017 16:02:19 GMT'}, {'version': 'v2', 'created': 'Thu, 16 Nov 2017 07:58:35 GMT'}, {'version': 'v3', 'created': 'Fri, 16 Feb 2018 08:53:44 GMT'}, {'version': 'v4', 'created': 'Thu, 22 Feb 2018 09:18:34 GMT'}, {'version': 'v5', 'created': 'Mon, 28 Jan 2019 04:39:30 GMT'}, {'version': 'v6', 'created': 'Fri, 3 May 2019 08:32:24 GMT'}, {'version': 'v7', 'created': 'Thu, 17 Oct 2019 07:46:53 GMT'}]",2019-10-18,"[['Su', 'Jiawei', ''], ['Vargas', 'Danilo Vasconcellos', ''], ['Kouichi', 'Sakurai', '']]",2019,10,18,One pixel attack for fooling deep neural networks,94,zscore: 4.132999,"Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97% of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47% confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.",Image ,,Adversarial,Attacks,Exposing Weaknesses,,,
39,1710.09412,Hongyi Zhang,"Hongyi Zhang, Moustapha Cisse, Yann N. Dauphin, David Lopez-Paz",mixup: Beyond Empirical Risk Minimization,"ICLR camera ready version. Changes vs V1: fix repo URL; add ablation
  studies; add mixup + dropout etc",,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Large deep neural networks are powerful, but exhibit undesirable behaviors
such as memorization and sensitivity to adversarial examples. In this work, we
propose mixup, a simple learning principle to alleviate these issues. In
essence, mixup trains a neural network on convex combinations of pairs of
examples and their labels. By doing so, mixup regularizes the neural network to
favor simple linear behavior in-between training examples. Our experiments on
the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show
that mixup improves the generalization of state-of-the-art neural network
architectures. We also find that mixup reduces the memorization of corrupt
labels, increases the robustness to adversarial examples, and stabilizes the
training of generative adversarial networks.
","[{'version': 'v1', 'created': 'Wed, 25 Oct 2017 18:30:49 GMT'}, {'version': 'v2', 'created': 'Fri, 27 Apr 2018 21:39:25 GMT'}]",2018-05-01,"[['Zhang', 'Hongyi', ''], ['Cisse', 'Moustapha', ''], ['Dauphin', 'Yann N.', ''], ['Lopez-Paz', 'David', '']]",2018,5,1,mixup: Beyond Empirical Risk Minimization,91,zscore: 4.004777,"Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.",Image,,Adversarial,Attacks,Exposing Weaknesses,Shielding,,
40,1710.10196,Samuli Laine,"Tero Karras, Timo Aila, Samuli Laine, Jaakko Lehtinen","Progressive Growing of GANs for Improved Quality, Stability, and
  Variation",Final ICLR 2018 version,,,,cs.NE cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We describe a new training methodology for generative adversarial networks.
The key idea is to grow both the generator and discriminator progressively:
starting from a low resolution, we add new layers that model increasingly fine
details as training progresses. This both speeds the training up and greatly
stabilizes it, allowing us to produce images of unprecedented quality, e.g.,
CelebA images at 1024^2. We also propose a simple way to increase the variation
in generated images, and achieve a record inception score of 8.80 in
unsupervised CIFAR10. Additionally, we describe several implementation details
that are important for discouraging unhealthy competition between the generator
and discriminator. Finally, we suggest a new metric for evaluating GAN results,
both in terms of image quality and variation. As an additional contribution, we
construct a higher-quality version of the CelebA dataset.
","[{'version': 'v1', 'created': 'Fri, 27 Oct 2017 15:28:35 GMT'}, {'version': 'v2', 'created': 'Fri, 3 Nov 2017 14:39:27 GMT'}, {'version': 'v3', 'created': 'Mon, 26 Feb 2018 15:33:34 GMT'}]",2018-02-28,"[['Karras', 'Tero', ''], ['Aila', 'Timo', ''], ['Laine', 'Samuli', ''], ['Lehtinen', 'Jaakko', '']]",2018,2,28,"Progressive Growing of GANs for Improved Quality, Stability, and   Variation",340,zscore: 16.409013,"We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.",Image ,,GAN,,Better accuracy,,,
41,1710.11041,Mikel Artetxe,"Mikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho",Unsupervised Neural Machine Translation,Published as a conference paper at ICLR 2018,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In spite of the recent success of neural machine translation (NMT) in
standard benchmarks, the lack of large parallel corpora poses a major practical
problem for many language pairs. There have been several proposals to alleviate
this issue with, for instance, triangulation and semi-supervised learning
techniques, but they still require a strong cross-lingual signal. In this work,
we completely remove the need of parallel data and propose a novel method to
train an NMT system in a completely unsupervised manner, relying on nothing but
monolingual corpora. Our model builds upon the recent work on unsupervised
embedding mappings, and consists of a slightly modified attentional
encoder-decoder model that can be trained on monolingual corpora alone using a
combination of denoising and backtranslation. Despite the simplicity of the
approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014
French-to-English and German-to-English translation. The model can also profit
from small parallel corpora, and attains 21.81 and 15.24 points when combined
with 100,000 parallel sentences, respectively. Our implementation is released
as an open source project.
","[{'version': 'v1', 'created': 'Mon, 30 Oct 2017 16:17:34 GMT'}, {'version': 'v2', 'created': 'Mon, 26 Feb 2018 16:54:14 GMT'}]",2018-02-27,"[['Artetxe', 'Mikel', ''], ['Labaka', 'Gorka', ''], ['Agirre', 'Eneko', ''], ['Cho', 'Kyunghyun', '']]",2018,2,27,Unsupervised Neural Machine Translation,87,zscore: 4.060069,"In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.",Text,,Generation,Seq2Seq,Fewer data,,,
42,1711.03938,Alexey Dosovitskiy,"Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez,
  Vladlen Koltun",CARLA: An Open Urban Driving Simulator,Published at the 1st Conference on Robot Learning (CoRL),,,,cs.LG cs.AI cs.CV cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We introduce CARLA, an open-source simulator for autonomous driving research.
CARLA has been developed from the ground up to support development, training,
and validation of autonomous urban driving systems. In addition to open-source
code and protocols, CARLA provides open digital assets (urban layouts,
buildings, vehicles) that were created for this purpose and can be used freely.
The simulation platform supports flexible specification of sensor suites and
environmental conditions. We use CARLA to study the performance of three
approaches to autonomous driving: a classic modular pipeline, an end-to-end
model trained via imitation learning, and an end-to-end model trained via
reinforcement learning. The approaches are evaluated in controlled scenarios of
increasing difficulty, and their performance is examined via metrics provided
by CARLA, illustrating the platform's utility for autonomous driving research.
The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E
","[{'version': 'v1', 'created': 'Fri, 10 Nov 2017 17:54:40 GMT'}]",2017-11-13,"[['Dosovitskiy', 'Alexey', ''], ['Ros', 'German', ''], ['Codevilla', 'Felipe', ''], ['Lopez', 'Antonio', ''], ['Koltun', 'Vladlen', '']]",2017,11,13,CARLA: An Open Urban Driving Simulator,99,zscore: 8.441420,"We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research. The supplementary video can be viewed at https://youtu.be/Hp8Dz-Zek2E",Autonomous driving,,Various,,Toolkit,,,
43,1711.03953,Zhilin Yang,"Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, William W. Cohen",Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,ICLR Oral 2018,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We formulate language modeling as a matrix factorization problem, and show
that the expressiveness of Softmax-based models (including the majority of
neural language models) is limited by a Softmax bottleneck. Given that natural
language is highly context-dependent, this further implies that in practice
Softmax with distributed word embeddings does not have enough capacity to model
natural language. We propose a simple and effective method to address this
issue, and improve the state-of-the-art perplexities on Penn Treebank and
WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on
the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points
in perplexity.
","[{'version': 'v1', 'created': 'Fri, 10 Nov 2017 18:29:00 GMT'}, {'version': 'v2', 'created': 'Mon, 13 Nov 2017 20:40:35 GMT'}, {'version': 'v3', 'created': 'Fri, 9 Feb 2018 01:15:08 GMT'}, {'version': 'v4', 'created': 'Fri, 2 Mar 2018 20:20:52 GMT'}]",2018-03-06,"[['Yang', 'Zhilin', ''], ['Dai', 'Zihang', ''], ['Salakhutdinov', 'Ruslan', ''], ['Cohen', 'William W.', '']]",2018,3,6,Breaking the Softmax Bottleneck: A High-Rank RNN Language Model,53,zscore: 4.281725,"We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively. The proposed method also excels on the large-scale 1B Word dataset, outperforming the baseline by over 5.6 points in perplexity.",Text,,Generation,"Language Modeling, Seq2Seq",Better accuracy,,,
44,1711.05144,Seth  Neel,"Michael Kearns, Seth Neel, Aaron Roth, Zhiwei Steven Wu","Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup
  Fairness","Added new experimental results and a slightly modified fairness
  definition",,,,cs.LG cs.DS cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The most prevalent notions of fairness in machine learning are statistical
definitions: they fix a small collection of pre-defined groups, and then ask
for parity of some statistic of the classifier across these groups. Constraints
of this form are susceptible to intentional or inadvertent ""fairness
gerrymandering"", in which a classifier appears to be fair on each individual
group, but badly violates the fairness constraint on one or more structured
subgroups defined over the protected attributes. We propose instead to demand
statistical notions of fairness across exponentially (or infinitely) many
subgroups, defined by a structured class of functions over the protected
attributes. This interpolates between statistical definitions of fairness and
recently proposed individual notions of fairness, but raises several
computational challenges. It is no longer clear how to audit a fixed classifier
to see if it satisfies such a strong definition of fairness. We prove that the
computational problem of auditing subgroup fairness for both equality of false
positive rates and statistical parity is equivalent to the problem of weak
agnostic learning, which means it is computationally hard in the worst case,
even for simple structured subclasses.
  We then derive two algorithms that provably converge to the best fair
classifier, given access to oracles which can solve the agnostic learning
problem. The algorithms are based on a formulation of subgroup fairness as a
two-player zero-sum game between a Learner and an Auditor. Our first algorithm
provably converges in a polynomial number of steps. Our second algorithm enjoys
only provably asymptotic convergence, but has the merit of simplicity and
faster per-step computation. We implement the simpler algorithm using linear
regression as a heuristic oracle, and show that we can effectively both audit
and learn fair classifiers on real datasets.
","[{'version': 'v1', 'created': 'Tue, 14 Nov 2017 15:34:27 GMT'}, {'version': 'v2', 'created': 'Wed, 15 Nov 2017 13:55:17 GMT'}, {'version': 'v3', 'created': 'Mon, 8 Jan 2018 01:15:28 GMT'}, {'version': 'v4', 'created': 'Thu, 12 Apr 2018 21:15:28 GMT'}, {'version': 'v5', 'created': 'Mon, 3 Dec 2018 18:18:34 GMT'}]",2018-12-04,"[['Kearns', 'Michael', ''], ['Neel', 'Seth', ''], ['Roth', 'Aaron', ''], ['Wu', 'Zhiwei Steven', '']]",2018,12,4,Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup   Fairness,41,zscore: 3.830730,"The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent ""fairness gerrymandering"", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses.   We then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.",Other,Fairness in ML,Other,,Fairness,,,
45,1711.05225,Pranav Rajpurkar,"Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel
  Mehta, Tony Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya,
  Matthew P. Lungren, Andrew Y. Ng","CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep
  Learning",,,,,cs.CV cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We develop an algorithm that can detect pneumonia from chest X-rays at a
level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer
convolutional neural network trained on ChestX-ray14, currently the largest
publicly available chest X-ray dataset, containing over 100,000 frontal-view
X-ray images with 14 diseases. Four practicing academic radiologists annotate a
test set, on which we compare the performance of CheXNet to that of
radiologists. We find that CheXNet exceeds average radiologist performance on
the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and
achieve state of the art results on all 14 diseases.
","[{'version': 'v1', 'created': 'Tue, 14 Nov 2017 17:58:50 GMT'}, {'version': 'v2', 'created': 'Sat, 25 Nov 2017 04:21:27 GMT'}, {'version': 'v3', 'created': 'Mon, 25 Dec 2017 11:09:06 GMT'}]",2017-12-27,"[['Rajpurkar', 'Pranav', ''], ['Irvin', 'Jeremy', ''], ['Zhu', 'Kaylie', ''], ['Yang', 'Brandon', ''], ['Mehta', 'Hershel', ''], ['Duan', 'Tony', ''], ['Ding', 'Daisy', ''], ['Bagul', 'Aarti', ''], ['Langlotz', 'Curtis', ''], ['Shpanskaya', 'Katie', ''], ['Lungren', 'Matthew P.', ''], ['Ng', 'Andrew Y.', '']]",2017,12,27,CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep   Learning,90,zscore: 9.003147,"We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest X-ray dataset, containing over 100,000 frontal-view X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.",Image ,,Deep Learning,Architectures /CNN),Better accuracy,,,
46,1711.10337,Mario Lucic,"Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, Olivier
  Bousquet",Are GANs Created Equal? A Large-Scale Study,"NIPS'18: Added a section on the limitations of the study and
  additional empirical results",,,,stat.ML cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generative adversarial networks (GAN) are a powerful subclass of generative
models. Despite a very rich research activity leading to numerous interesting
GAN algorithms, it is still very hard to assess which algorithm(s) perform
better than others. We conduct a neutral, multi-faceted large-scale empirical
study on state-of-the art models and evaluation measures. We find that most
models can reach similar scores with enough hyperparameter optimization and
random restarts. This suggests that improvements can arise from a higher
computational budget and tuning more than fundamental algorithmic changes. To
overcome some limitations of the current metrics, we also propose several data
sets on which precision and recall can be computed. Our experimental results
suggest that future GAN research should be based on more systematic and
objective evaluation procedures. Finally, we did not find evidence that any of
the tested algorithms consistently outperforms the non-saturating GAN
introduced in \cite{goodfellow2014generative}.
","[{'version': 'v1', 'created': 'Tue, 28 Nov 2017 15:19:53 GMT'}, {'version': 'v2', 'created': 'Thu, 30 Nov 2017 17:09:16 GMT'}, {'version': 'v3', 'created': 'Tue, 20 Mar 2018 09:18:08 GMT'}, {'version': 'v4', 'created': 'Mon, 29 Oct 2018 15:34:15 GMT'}]",2018-10-30,"[['Lucic', 'Mario', ''], ['Kurach', 'Karol', ''], ['Michalski', 'Marcin', ''], ['Gelly', 'Sylvain', ''], ['Bousquet', 'Olivier', '']]",2018,10,30,Are GANs Created Equal? A Large-Scale Study,85,zscore: 6.570384,"Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \cite{goodfellow2014generative}.",Various,,GAN,,Review,,,
47,1711.10433,"A\""aron van den Oord","Aaron van den Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan, Oriol
  Vinyals, Koray Kavukcuoglu, George van den Driessche, Edward Lockhart, Luis
  C. Cobo, Florian Stimberg, Norman Casagrande, Dominik Grewe, Seb Noury,
  Sander Dieleman, Erich Elsen, Nal Kalchbrenner, Heiga Zen, Alex Graves, Helen
  King, Tom Walters, Dan Belov, Demis Hassabis",Parallel WaveNet: Fast High-Fidelity Speech Synthesis,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The recently-developed WaveNet architecture is the current state of the art
in realistic speech synthesis, consistently rated as more natural sounding for
many different languages than any previous system. However, because WaveNet
relies on sequential generation of one audio sample at a time, it is poorly
suited to today's massively parallel computers, and therefore hard to deploy in
a real-time production setting. This paper introduces Probability Density
Distillation, a new method for training a parallel feed-forward network from a
trained WaveNet with no significant difference in quality. The resulting system
is capable of generating high-fidelity speech samples at more than 20 times
faster than real-time, and is deployed online by Google Assistant, including
serving multiple English and Japanese voices.
","[{'version': 'v1', 'created': 'Tue, 28 Nov 2017 17:48:11 GMT'}]",2017-11-29,"[['Oord', 'Aaron van den', ''], ['Li', 'Yazhe', ''], ['Babuschkin', 'Igor', ''], ['Simonyan', 'Karen', ''], ['Vinyals', 'Oriol', ''], ['Kavukcuoglu', 'Koray', ''], ['Driessche', 'George van den', ''], ['Lockhart', 'Edward', ''], ['Cobo', 'Luis C.', ''], ['Stimberg', 'Florian', ''], ['Casagrande', 'Norman', ''], ['Grewe', 'Dominik', ''], ['Noury', 'Seb', ''], ['Dieleman', 'Sander', ''], ['Elsen', 'Erich', ''], ['Kalchbrenner', 'Nal', ''], ['Zen', 'Heiga', ''], ['Graves', 'Alex', ''], ['King', 'Helen', ''], ['Walters', 'Tom', ''], ['Belov', 'Dan', ''], ['Hassabis', 'Demis', '']]",2017,11,29,Parallel WaveNet: Fast High-Fidelity Speech Synthesis,64,zscore: 4.843620,"The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.",Speech,Synthesis,Distillation,Probability Density Distillation,Faster,,,
48,1711.11585,Ting-Chun Wang,"Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, Bryan
  Catanzaro","High-Resolution Image Synthesis and Semantic Manipulation with
  Conditional GANs","v2: CVPR camera ready, adding more results for edge-to-photo examples",,,,cs.CV cs.GR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a new method for synthesizing high-resolution photo-realistic
images from semantic label maps using conditional generative adversarial
networks (conditional GANs). Conditional GANs have enabled a variety of
applications, but the results are often limited to low-resolution and still far
from realistic. In this work, we generate 2048x1024 visually appealing results
with a novel adversarial loss, as well as new multi-scale generator and
discriminator architectures. Furthermore, we extend our framework to
interactive visual manipulation with two additional features. First, we
incorporate object instance segmentation information, which enables object
manipulations such as removing/adding objects and changing the object category.
Second, we propose a method to generate diverse results given the same input,
allowing users to edit the object appearance interactively. Human opinion
studies demonstrate that our method significantly outperforms existing methods,
advancing both the quality and the resolution of deep image synthesis and
editing.
","[{'version': 'v1', 'created': 'Thu, 30 Nov 2017 18:57:21 GMT'}, {'version': 'v2', 'created': 'Mon, 20 Aug 2018 17:55:56 GMT'}]",2018-08-21,"[['Wang', 'Ting-Chun', ''], ['Liu', 'Ming-Yu', ''], ['Zhu', 'Jun-Yan', ''], ['Tao', 'Andrew', ''], ['Kautz', 'Jan', ''], ['Catanzaro', 'Bryan', '']]",2018,8,21,High-Resolution Image Synthesis and Semantic Manipulation with   Conditional GANs,122,zscore: 9.521408,"We present a new method for synthesizing high-resolution photo-realistic images from semantic label maps using conditional generative adversarial networks (conditional GANs). Conditional GANs have enabled a variety of applications, but the results are often limited to low-resolution and still far from realistic. In this work, we generate 2048x1024 visually appealing results with a novel adversarial loss, as well as new multi-scale generator and discriminator architectures. Furthermore, we extend our framework to interactive visual manipulation with two additional features. First, we incorporate object instance segmentation information, which enables object manipulations such as removing/adding objects and changing the object category. Second, we propose a method to generate diverse results given the same input, allowing users to edit the object appearance interactively. Human opinion studies demonstrate that our method significantly outperforms existing methods, advancing both the quality and the resolution of deep image synthesis and editing.",Image ,,GAN,,Better accuracy,,,
49,1712.00559,Chenxi Liu,"Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua,
  Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, Kevin Murphy",Progressive Neural Architecture Search,"To appear in ECCV 2018 as oral. The code and checkpoint for PNASNet-5
  trained on ImageNet (both Mobile and Large) can now be downloaded from
  https://github.com/tensorflow/models/tree/master/research/slim#Pretrained.
  Also see https://github.com/chenxi116/PNASNet.TF for refactored and
  simplified TensorFlow code; see https://github.com/chenxi116/PNASNet.pytorch
  for exact conversion to PyTorch",,,,cs.CV cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a new method for learning the structure of convolutional neural
networks (CNNs) that is more efficient than recent state-of-the-art methods
based on reinforcement learning and evolutionary algorithms. Our approach uses
a sequential model-based optimization (SMBO) strategy, in which we search for
structures in order of increasing complexity, while simultaneously learning a
surrogate model to guide the search through structure space. Direct comparison
under the same search space shows that our method is up to 5 times more
efficient than the RL method of Zoph et al. (2018) in terms of number of models
evaluated, and 8 times faster in terms of total compute. The structures we
discover in this way achieve state of the art classification accuracies on
CIFAR-10 and ImageNet.
","[{'version': 'v1', 'created': 'Sat, 2 Dec 2017 06:23:16 GMT'}, {'version': 'v2', 'created': 'Sat, 24 Mar 2018 00:39:27 GMT'}, {'version': 'v3', 'created': 'Thu, 26 Jul 2018 19:51:26 GMT'}]",2018-07-30,"[['Liu', 'Chenxi', ''], ['Zoph', 'Barret', ''], ['Neumann', 'Maxim', ''], ['Shlens', 'Jonathon', ''], ['Hua', 'Wei', ''], ['Li', 'Li-Jia', ''], ['Fei-Fei', 'Li', ''], ['Yuille', 'Alan', ''], ['Huang', 'Jonathan', ''], ['Murphy', 'Kevin', '']]",2018,7,30,Progressive Neural Architecture Search,76,zscore: 5.544109,"We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.",General,,Architecture Search,,Faster,,,
50,1712.01815,David Silver,"David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
  Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran,
  Thore Graepel, Timothy Lillicrap, Karen Simonyan, Demis Hassabis","Mastering Chess and Shogi by Self-Play with a General Reinforcement
  Learning Algorithm",,,,,cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The game of chess is the most widely-studied domain in the history of
artificial intelligence. The strongest programs are based on a combination of
sophisticated search techniques, domain-specific adaptations, and handcrafted
evaluation functions that have been refined by human experts over several
decades. In contrast, the AlphaGo Zero program recently achieved superhuman
performance in the game of Go, by tabula rasa reinforcement learning from games
of self-play. In this paper, we generalise this approach into a single
AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in
many challenging domains. Starting from random play, and given no domain
knowledge except the game rules, AlphaZero achieved within 24 hours a
superhuman level of play in the games of chess and shogi (Japanese chess) as
well as Go, and convincingly defeated a world-champion program in each case.
","[{'version': 'v1', 'created': 'Tue, 5 Dec 2017 18:45:38 GMT'}]",2017-12-06,"[['Silver', 'David', ''], ['Hubert', 'Thomas', ''], ['Schrittwieser', 'Julian', ''], ['Antonoglou', 'Ioannis', ''], ['Lai', 'Matthew', ''], ['Guez', 'Arthur', ''], ['Lanctot', 'Marc', ''], ['Sifre', 'Laurent', ''], ['Kumaran', 'Dharshan', ''], ['Graepel', 'Thore', ''], ['Lillicrap', 'Timothy', ''], ['Simonyan', 'Karen', ''], ['Hassabis', 'Demis', '']]",2017,12,6,Mastering Chess and Shogi by Self-Play with a General Reinforcement   Learning Algorithm,110,zscore: 7.966731,"The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.",Games,,Reinforcement Learning,,Better accuracy,,,
51,1712.04248,Jonas Rauber,"Wieland Brendel, Jonas Rauber, Matthias Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box
  Machine Learning Models","Published as a conference paper at the Sixth International Conference
  on Learning Representations (ICLR 2018)
  https://openreview.net/forum?id=SyZI0GWCZ",,,,stat.ML cs.CR cs.CV cs.LG cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many machine learning algorithms are vulnerable to almost imperceptible
perturbations of their inputs. So far it was unclear how much risk adversarial
perturbations carry for the safety of real-world machine learning applications
because most methods used to generate such perturbations rely either on
detailed model information (gradient-based attacks) or on confidence scores
such as class probabilities (score-based attacks), neither of which are
available in most real-world scenarios. In many such cases one currently needs
to retreat to transfer-based attacks which rely on cumbersome substitute
models, need access to the training data and can be defended against. Here we
emphasise the importance of attacks which solely rely on the final model
decision. Such decision-based attacks are (1) applicable to real-world
black-box models such as autonomous cars, (2) need less knowledge and are
easier to apply than transfer-based attacks and (3) are more robust to simple
defences than gradient- or score-based attacks. Previous attacks in this
category were limited to simple models or simple datasets. Here we introduce
the Boundary Attack, a decision-based attack that starts from a large
adversarial perturbation and then seeks to reduce the perturbation while
staying adversarial. The attack is conceptually simple, requires close to no
hyperparameter tuning, does not rely on substitute models and is competitive
with the best gradient-based attacks in standard computer vision tasks like
ImageNet. We apply the attack on two black-box algorithms from Clarifai.com.
The Boundary Attack in particular and the class of decision-based attacks in
general open new avenues to study the robustness of machine learning models and
raise new questions regarding the safety of deployed machine learning systems.
An implementation of the attack is available as part of Foolbox at
https://github.com/bethgelab/foolbox .
","[{'version': 'v1', 'created': 'Tue, 12 Dec 2017 11:36:26 GMT'}, {'version': 'v2', 'created': 'Fri, 16 Feb 2018 14:40:42 GMT'}]",2018-02-19,"[['Brendel', 'Wieland', ''], ['Rauber', 'Jonas', ''], ['Bethge', 'Matthias', '']]",2018,2,19,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box   Machine Learning Models,45,zscore: 3.610399,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .",Image,,Adversarial,Attacks,Better accuracy,,,
52,1712.05690,Tobias Domhan,"Felix Hieber, Tobias Domhan, Michael Denkowski, David Vilar, Artem
  Sokolov, Ann Clifton, Matt Post",Sockeye: A Toolkit for Neural Machine Translation,,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We describe Sockeye (version 1.12), an open-source sequence-to-sequence
toolkit for Neural Machine Translation (NMT). Sockeye is a production-ready
framework for training and applying models as well as an experimental platform
for researchers. Written in Python and built on MXNet, the toolkit offers
scalable training and inference for the three most prominent encoder-decoder
architectures: attentional recurrent neural networks, self-attentional
transformers, and fully convolutional networks. Sockeye also supports a wide
range of optimizers, normalization and regularization techniques, and inference
improvements from current NMT literature. Users can easily run standard
training recipes, explore different model settings, and incorporate new ideas.
In this paper, we highlight Sockeye's features and benchmark it against other
NMT toolkits on two language arcs from the 2017 Conference on Machine
Translation (WMT): English-German and Latvian-English. We report competitive
BLEU scores across all three architectures, including an overall best score for
Sockeye's transformer implementation. To facilitate further comparison, we
release all system outputs and training scripts used in our experiments. The
Sockeye toolkit is free software released under the Apache 2.0 license.
","[{'version': 'v1', 'created': 'Fri, 15 Dec 2017 14:44:28 GMT'}, {'version': 'v2', 'created': 'Fri, 1 Jun 2018 13:29:31 GMT'}]",2018-06-04,"[['Hieber', 'Felix', ''], ['Domhan', 'Tobias', ''], ['Denkowski', 'Michael', ''], ['Vilar', 'David', ''], ['Sokolov', 'Artem', ''], ['Clifton', 'Ann', ''], ['Post', 'Matt', '']]",2018,6,4,Sockeye: A Toolkit for Neural Machine Translation,48,zscore: 3.998110,"We describe Sockeye (version 1.12), an open-source sequence-to-sequence toolkit for Neural Machine Translation (NMT). Sockeye is a production-ready framework for training and applying models as well as an experimental platform for researchers. Written in Python and built on MXNet, the toolkit offers scalable training and inference for the three most prominent encoder-decoder architectures: attentional recurrent neural networks, self-attentional transformers, and fully convolutional networks. Sockeye also supports a wide range of optimizers, normalization and regularization techniques, and inference improvements from current NMT literature. Users can easily run standard training recipes, explore different model settings, and incorporate new ideas. In this paper, we highlight Sockeye's features and benchmark it against other NMT toolkits on two language arcs from the 2017 Conference on Machine Translation (WMT): English-German and Latvian-English. We report competitive BLEU scores across all three architectures, including an overall best score for Sockeye's transformer implementation. To facilitate further comparison, we release all system outputs and training scripts used in our experiments. The Sockeye toolkit is free software released under the Apache 2.0 license.",Text,,Generation,Seq2Seq,Toolkit,,,
53,1712.06567,Felipe Petroski Such,"Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman,
  Kenneth O. Stanley, Jeff Clune","Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative
  for Training Deep Neural Networks for Reinforcement Learning",,,,,cs.NE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep artificial neural networks (DNNs) are typically trained via
gradient-based learning algorithms, namely backpropagation. Evolution
strategies (ES) can rival backprop-based algorithms such as Q-learning and
policy gradients on challenging deep reinforcement learning (RL) problems.
However, ES can be considered a gradient-based algorithm because it performs
stochastic gradient descent via an operation similar to a finite-difference
approximation of the gradient. That raises the question of whether
non-gradient-based evolutionary algorithms can work at DNN scales. Here we
demonstrate they can: we evolve the weights of a DNN with a simple,
gradient-free, population-based genetic algorithm (GA) and it performs well on
hard deep RL problems, including Atari and humanoid locomotion. The Deep GA
successfully evolves networks with over four million free parameters, the
largest neural networks ever evolved with a traditional evolutionary algorithm.
These results (1) expand our sense of the scale at which GAs can operate, (2)
suggest intriguingly that in some cases following the gradient is not the best
choice for optimizing performance, and (3) make immediately available the
multitude of neuroevolution techniques that improve performance. We demonstrate
the latter by showing that combining DNNs with novelty search, which encourages
exploration on tasks with deceptive or sparse reward functions, can solve a
high-dimensional problem on which reward-maximizing algorithms (e.g.\ DQN, A3C,
ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN
(it can train Atari in ${\raise.17ex\hbox{$\scriptstyle\sim$}}$4 hours on one
desktop or ${\raise.17ex\hbox{$\scriptstyle\sim$}}$1 hour distributed on 720
cores), and enables a state-of-the-art, up to 10,000-fold compact encoding
technique.
","[{'version': 'v1', 'created': 'Mon, 18 Dec 2017 18:22:05 GMT'}, {'version': 'v2', 'created': 'Thu, 4 Jan 2018 22:59:49 GMT'}, {'version': 'v3', 'created': 'Fri, 20 Apr 2018 18:38:34 GMT'}]",2018-04-24,"[['Such', 'Felipe Petroski', ''], ['Madhavan', 'Vashisht', ''], ['Conti', 'Edoardo', ''], ['Lehman', 'Joel', ''], ['Stanley', 'Kenneth O.', ''], ['Clune', 'Jeff', '']]",2018,4,24,Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative   for Training Deep Neural Networks for Reinforcement Learning,76,zscore: 7.917699,"Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.\ DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in ${\raise.17ex\hbox{$\scriptstyle\sim$}}$4 hours on one desktop or ${\raise.17ex\hbox{$\scriptstyle\sim$}}$1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.",Various,,Reinforcement Learning,,Better accuracy,,,
54,1712.07107,Xiaoyong Yuan,"Xiaoyong Yuan, Pan He, Qile Zhu, Xiaolin Li",Adversarial Examples: Attacks and Defenses for Deep Learning,Github: https://github.com/chbrian/awesome-adversarial-examples-dl,,,,cs.LG cs.CR cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  With rapid progress and significant successes in a wide spectrum of
applications, deep learning is being applied in many safety-critical
environments. However, deep neural networks have been recently found vulnerable
to well-designed input samples, called adversarial examples. Adversarial
examples are imperceptible to human but can easily fool deep neural networks in
the testing/deploying stage. The vulnerability to adversarial examples becomes
one of the major risks for applying deep neural networks in safety-critical
environments. Therefore, attacks and defenses on adversarial examples draw
great attention. In this paper, we review recent findings on adversarial
examples for deep neural networks, summarize the methods for generating
adversarial examples, and propose a taxonomy of these methods. Under the
taxonomy, applications for adversarial examples are investigated. We further
elaborate on countermeasures for adversarial examples and explore the
challenges and the potential solutions.
","[{'version': 'v1', 'created': 'Tue, 19 Dec 2017 18:44:07 GMT'}, {'version': 'v2', 'created': 'Fri, 5 Jan 2018 15:51:54 GMT'}, {'version': 'v3', 'created': 'Sat, 7 Jul 2018 02:32:57 GMT'}]",2018-07-10,"[['Yuan', 'Xiaoyong', ''], ['He', 'Pan', ''], ['Zhu', 'Qile', ''], ['Li', 'Xiaolin', '']]",2018,7,10,Adversarial Examples: Attacks and Defenses for Deep Learning,46,zscore: 4.784302,"With rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. However, deep neural networks have been recently found vulnerable to well-designed input samples, called adversarial examples. Adversarial examples are imperceptible to human but can easily fool deep neural networks in the testing/deploying stage. The vulnerability to adversarial examples becomes one of the major risks for applying deep neural networks in safety-critical environments. Therefore, attacks and defenses on adversarial examples draw great attention. In this paper, we review recent findings on adversarial examples for deep neural networks, summarize the methods for generating adversarial examples, and propose a taxonomy of these methods. Under the taxonomy, applications for adversarial examples are investigated. We further elaborate on countermeasures for adversarial examples and explore the challenges and the potential solutions.",General,,Adversarial,Attacks,Review,,,
55,1712.09913,Hao Li,"Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, Tom Goldstein",Visualizing the Loss Landscape of Neural Nets,"NIPS 2018 (extended version, 10.5 pages), code is available at
  https://github.com/tomgoldstein/loss-landscape",,,,cs.LG cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural network training relies on our ability to find ""good"" minimizers of
highly non-convex loss functions. It is well-known that certain network
architecture designs (e.g., skip connections) produce loss functions that train
easier, and well-chosen training parameters (batch size, learning rate,
optimizer) produce minimizers that generalize better. However, the reasons for
these differences, and their effects on the underlying loss landscape, are not
well understood. In this paper, we explore the structure of neural loss
functions, and the effect of loss landscapes on generalization, using a range
of visualization methods. First, we introduce a simple ""filter normalization""
method that helps us visualize loss function curvature and make meaningful
side-by-side comparisons between loss functions. Then, using a variety of
visualizations, we explore how network architecture affects the loss landscape,
and how training parameters affect the shape of minimizers.
","[{'version': 'v1', 'created': 'Thu, 28 Dec 2017 16:15:42 GMT'}, {'version': 'v2', 'created': 'Mon, 5 Mar 2018 18:23:03 GMT'}, {'version': 'v3', 'created': 'Wed, 7 Nov 2018 06:25:20 GMT'}]",2018-11-08,"[['Li', 'Hao', ''], ['Xu', 'Zheng', ''], ['Taylor', 'Gavin', ''], ['Studer', 'Christoph', ''], ['Goldstein', 'Tom', '']]",2018,11,8,Visualizing the Loss Landscape of Neural Nets,51,zscore: 4.974303,"Neural network training relies on our ability to find ""good"" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple ""filter normalization"" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",Theoretical,,Learning,Aspects of DL,Evaluation,,,
56,1801.00631,Gary Marcus,Gary Marcus,Deep Learning: A Critical Appraisal,1 figure,,,,cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Although deep learning has historical roots going back decades, neither the
term ""deep learning"" nor the approach was popular just over five years ago,
when the field was reignited by papers such as Krizhevsky, Sutskever and
Hinton's now classic (2012) deep network model of Imagenet. What has the field
discovered in the five subsequent years? Against a background of considerable
progress in areas such as speech recognition, image recognition, and game
playing, and considerable enthusiasm in the popular press, I present ten
concerns for deep learning, and suggest that deep learning must be supplemented
by other techniques if we are to reach artificial general intelligence.
","[{'version': 'v1', 'created': 'Tue, 2 Jan 2018 12:49:35 GMT'}]",2018-01-04,"[['Marcus', 'Gary', '']]",2018,1,4,Deep Learning: A Critical Appraisal,60,zscore: 7.002704,"Although deep learning has historical roots going back decades, neither the term ""deep learning"" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.",Other,None,Deep Learning,,Exposing Weaknesses,,,
57,1801.01944,Nicholas Carlini,"Nicholas Carlini, David Wagner",Audio Adversarial Examples: Targeted Attacks on Speech-to-Text,,,,,cs.LG cs.AI cs.CR,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We construct targeted audio adversarial examples on automatic speech
recognition. Given any audio waveform, we can produce another that is over
99.9% similar, but transcribes as any phrase we choose (recognizing up to 50
characters per second of audio). We apply our white-box iterative
optimization-based attack to Mozilla's implementation DeepSpeech end-to-end,
and show it has a 100% success rate. The feasibility of this attack introduce a
new domain to study adversarial examples.
","[{'version': 'v1', 'created': 'Fri, 5 Jan 2018 23:40:04 GMT'}, {'version': 'v2', 'created': 'Fri, 30 Mar 2018 02:06:30 GMT'}]",2018-04-02,"[['Carlini', 'Nicholas', ''], ['Wagner', 'David', '']]",2018,4,2,Audio Adversarial Examples: Targeted Attacks on Speech-to-Text,57,zscore: 6.785934,"We construct targeted audio adversarial examples on automatic speech recognition. Given any audio waveform, we can produce another that is over 99.9% similar, but transcribes as any phrase we choose (recognizing up to 50 characters per second of audio). We apply our white-box iterative optimization-based attack to Mozilla's implementation DeepSpeech end-to-end, and show it has a 100% success rate. The feasibility of this attack introduce a new domain to study adversarial examples.",Speech,,Adversarial,Attacks,Exposing Weaknesses,,,
58,1801.04406,Lars Mescheder,"Lars Mescheder, Andreas Geiger, Sebastian Nowozin",Which Training Methods for GANs do actually Converge?,conference,International Conference on Machine Learning 2018,,,cs.LG cs.AI cs.GT,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has shown local convergence of GAN training for absolutely
continuous data and generator distributions. In this paper, we show that the
requirement of absolute continuity is necessary: we describe a simple yet
prototypical counterexample showing that in the more realistic case of
distributions that are not absolutely continuous, unregularized GAN training is
not always convergent. Furthermore, we discuss regularization strategies that
were recently proposed to stabilize GAN training. Our analysis shows that GAN
training with instance noise or zero-centered gradient penalties converges. On
the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number
of discriminator updates per generator update do not always converge to the
equilibrium point. We discuss these results, leading us to a new explanation
for the stability problems of GAN training. Based on our analysis, we extend
our convergence results to more general GANs and prove local convergence for
simplified gradient penalties even if the generator and data distribution lie
on lower dimensional manifolds. We find these penalties to work well in
practice and use them to learn high-resolution generative image models for a
variety of datasets with little hyperparameter tuning.
","[{'version': 'v1', 'created': 'Sat, 13 Jan 2018 09:42:26 GMT'}, {'version': 'v2', 'created': 'Mon, 19 Feb 2018 10:40:54 GMT'}, {'version': 'v3', 'created': 'Mon, 11 Jun 2018 15:39:06 GMT'}, {'version': 'v4', 'created': 'Tue, 31 Jul 2018 16:28:15 GMT'}]",2018-08-01,"[['Mescheder', 'Lars', ''], ['Geiger', 'Andreas', ''], ['Nowozin', 'Sebastian', '']]",2018,8,1,Which Training Methods for GANs do actually Converge?,33,zscore: 4.734960,"Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.",Image ,,GAN,,Better accuracy,,,
59,1801.07736,William Fedus,"William Fedus, Ian Goodfellow and Andrew M. Dai",MaskGAN: Better Text Generation via Filling in the______,"16 pages, ICLR 2018",,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural text generation models are often autoregressive language models or
seq2seq models. These models generate text by sampling words sequentially, with
each word conditioned on the previous word, and are state-of-the-art for
several machine translation and summarization benchmarks. These benchmarks are
often defined by validation perplexity even though this is not a direct measure
of the quality of the generated text. Additionally, these models are typically
trained via maxi- mum likelihood and teacher forcing. These methods are
well-suited to optimizing perplexity but can result in poor sample quality
since generating text requires conditioning on sequences of words that may have
never been observed at training time. We propose to improve sample quality
using Generative Adversarial Networks (GANs), which explicitly train the
generator to produce high quality samples and have shown a lot of success in
image generation. GANs were originally designed to output differentiable
values, so discrete language generation is challenging for them. We claim that
validation perplexity alone is not indicative of the quality of text generated
by a model. We introduce an actor-critic conditional GAN that fills in missing
text conditioned on the surrounding context. We show qualitatively and
quantitatively, evidence that this produces more realistic conditional and
unconditional text samples compared to a maximum likelihood trained model.
","[{'version': 'v1', 'created': 'Tue, 23 Jan 2018 19:22:21 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Feb 2018 16:26:04 GMT'}, {'version': 'v3', 'created': 'Thu, 1 Mar 2018 15:30:09 GMT'}]",2018-03-02,"[['Fedus', 'William', ''], ['Goodfellow', 'Ian', ''], ['Dai', 'Andrew M.', '']]",2018,3,2,MaskGAN: Better Text Generation via Filling in the______,37,zscore: 6.179971,"Neural text generation models are often autoregressive language models or seq2seq models. These models generate text by sampling words sequentially, with each word conditioned on the previous word, and are state-of-the-art for several machine translation and summarization benchmarks. These benchmarks are often defined by validation perplexity even though this is not a direct measure of the quality of the generated text. Additionally, these models are typically trained via maxi- mum likelihood and teacher forcing. These methods are well-suited to optimizing perplexity but can result in poor sample quality since generating text requires conditioning on sequences of words that may have never been observed at training time. We propose to improve sample quality using Generative Adversarial Networks (GANs), which explicitly train the generator to produce high quality samples and have shown a lot of success in image generation. GANs were originally designed to output differentiable values, so discrete language generation is challenging for them. We claim that validation perplexity alone is not indicative of the quality of text generated by a model. We introduce an actor-critic conditional GAN that fills in missing text conditioned on the surrounding context. We show qualitatively and quantitatively, evidence that this produces more realistic conditional and unconditional text samples compared to a maximum likelihood trained model.",Text,,GAN,,Better accuracy,,,
60,1801.07860,Eyal Oren,"Alvin Rajkomar, Eyal Oren, Kai Chen, Andrew M. Dai, Nissan Hajaj,
  Peter J. Liu, Xiaobing Liu, Mimi Sun, Patrik Sundberg, Hector Yee, Kun Zhang,
  Gavin E. Duggan, Gerardo Flores, Michaela Hardt, Jamie Irvine, Quoc Le, Kurt
  Litsch, Jake Marcus, Alexander Mossin, Justin Tansuwan, De Wang, James
  Wexler, Jimbo Wilson, Dana Ludwig, Samuel L. Volchenboum, Katherine Chou,
  Michael Pearson, Srinivasan Madabushi, Nigam H. Shah, Atul J. Butte, Michael
  Howell, Claire Cui, Greg Corrado, Jeff Dean",Scalable and accurate deep learning for electronic health records,"Published version from
  https://www.nature.com/articles/s41746-018-0029-1",npj Digital Medicine 1:18 (2018),10.1038/s41746-018-0029-1,,cs.CY cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Predictive modeling with electronic health record (EHR) data is anticipated
to drive personalized medicine and improve healthcare quality. Constructing
predictive statistical models typically requires extraction of curated
predictor variables from normalized EHR data, a labor-intensive process that
discards the vast majority of information in each patient's record. We propose
a representation of patients' entire, raw EHR records based on the Fast
Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep
learning methods using this representation are capable of accurately predicting
multiple medical events from multiple centers without site-specific data
harmonization. We validated our approach using de-identified EHR data from two
U.S. academic medical centers with 216,221 adult patients hospitalized for at
least 24 hours. In the sequential format we propose, this volume of EHR data
unrolled into a total of 46,864,534,945 data points, including clinical notes.
Deep learning models achieved high accuracy for tasks such as predicting
in-hospital mortality (AUROC across sites 0.93-0.94), 30-day unplanned
readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and
all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90).
These models outperformed state-of-the-art traditional predictive models in all
cases. We also present a case-study of a neural-network attribution system,
which illustrates how clinicians can gain some transparency into the
predictions. We believe that this approach can be used to create accurate and
scalable predictions for a variety of clinical scenarios, complete with
explanations that directly highlight evidence in the patient's chart.
","[{'version': 'v1', 'created': 'Wed, 24 Jan 2018 05:06:43 GMT'}, {'version': 'v2', 'created': 'Fri, 26 Jan 2018 18:57:00 GMT'}, {'version': 'v3', 'created': 'Fri, 11 May 2018 12:16:50 GMT'}]",2018-05-14,"[['Rajkomar', 'Alvin', ''], ['Oren', 'Eyal', ''], ['Chen', 'Kai', ''], ['Dai', 'Andrew M.', ''], ['Hajaj', 'Nissan', ''], ['Liu', 'Peter J.', ''], ['Liu', 'Xiaobing', ''], ['Sun', 'Mimi', ''], ['Sundberg', 'Patrik', ''], ['Yee', 'Hector', ''], ['Zhang', 'Kun', ''], ['Duggan', 'Gavin E.', ''], ['Flores', 'Gerardo', ''], ['Hardt', 'Michaela', ''], ['Irvine', 'Jamie', ''], ['Le', 'Quoc', ''], ['Litsch', 'Kurt', ''], ['Marcus', 'Jake', ''], ['Mossin', 'Alexander', ''], ['Tansuwan', 'Justin', ''], ['Wang', 'De', ''], ['Wexler', 'James', ''], ['Wilson', 'Jimbo', ''], ['Ludwig', 'Dana', ''], ['Volchenboum', 'Samuel L.', ''], ['Chou', 'Katherine', ''], ['Pearson', 'Michael', ''], ['Madabushi', 'Srinivasan', ''], ['Shah', 'Nigam H.', ''], ['Butte', 'Atul J.', ''], ['Howell', 'Michael', ''], ['Cui', 'Claire', ''], ['Corrado', 'Greg', ''], ['Dean', 'Jeff', '']]",2018,5,14,Scalable and accurate deep learning for electronic health records,24,zscore: 3.817276,"Predictive modeling with electronic health record (EHR) data is anticipated to drive personalized medicine and improve healthcare quality. Constructing predictive statistical models typically requires extraction of curated predictor variables from normalized EHR data, a labor-intensive process that discards the vast majority of information in each patient's record. We propose a representation of patients' entire, raw EHR records based on the Fast Healthcare Interoperability Resources (FHIR) format. We demonstrate that deep learning methods using this representation are capable of accurately predicting multiple medical events from multiple centers without site-specific data harmonization. We validated our approach using de-identified EHR data from two U.S. academic medical centers with 216,221 adult patients hospitalized for at least 24 hours. In the sequential format we propose, this volume of EHR data unrolled into a total of 46,864,534,945 data points, including clinical notes. Deep learning models achieved high accuracy for tasks such as predicting in-hospital mortality (AUROC across sites 0.93-0.94), 30-day unplanned readmission (AUROC 0.75-0.76), prolonged length of stay (AUROC 0.85-0.86), and all of a patient's final discharge diagnoses (frequency-weighted AUROC 0.90). These models outperformed state-of-the-art traditional predictive models in all cases. We also present a case-study of a neural-network attribution system, which illustrates how clinicians can gain some transparency into the predictions. We believe that this approach can be used to create accurate and scalable predictions for a variety of clinical scenarios, complete with explanations that directly highlight evidence in the patient's chart.",Other,Health,Deep Learning,,Better accuracy,,,
61,1802.00923,Paul Pu Liang,"Amir Zadeh, Paul Pu Liang, Soujanya Poria, Prateek Vij, Erik Cambria,
  Louis-Philippe Morency",Multi-attention Recurrent Network for Human Communication Comprehension,AAAI 2018 Oral Presentation,,,,cs.AI cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Human face-to-face communication is a complex multimodal signal. We use words
(language modality), gestures (vision modality) and changes in tone (acoustic
modality) to convey our intentions. Humans easily process and understand
face-to-face communication, however, comprehending this form of communication
remains a significant challenge for Artificial Intelligence (AI). AI must
understand each modality and the interactions between them that shape human
communication. In this paper, we present a novel neural architecture for
understanding human communication called the Multi-attention Recurrent Network
(MARN). The main strength of our model comes from discovering interactions
between modalities through time using a neural component called the
Multi-attention Block (MAB) and storing them in the hybrid memory of a
recurrent component called the Long-short Term Hybrid Memory (LSTHM). We
perform extensive comparisons on six publicly available datasets for multimodal
sentiment analysis, speaker trait recognition and emotion recognition. MARN
shows state-of-the-art performance on all the datasets.
","[{'version': 'v1', 'created': 'Sat, 3 Feb 2018 06:29:17 GMT'}]",2018-02-06,"[['Zadeh', 'Amir', ''], ['Liang', 'Paul Pu', ''], ['Poria', 'Soujanya', ''], ['Vij', 'Prateek', ''], ['Cambria', 'Erik', ''], ['Morency', 'Louis-Philippe', '']]",2018,2,6,Multi-attention Recurrent Network for Human Communication Comprehension,29,zscore: 4.558519,"Human face-to-face communication is a complex multimodal signal. We use words (language modality), gestures (vision modality) and changes in tone (acoustic modality) to convey our intentions. Humans easily process and understand face-to-face communication, however, comprehending this form of communication remains a significant challenge for Artificial Intelligence (AI). AI must understand each modality and the interactions between them that shape human communication. In this paper, we present a novel neural architecture for understanding human communication called the Multi-attention Recurrent Network (MARN). The main strength of our model comes from discovering interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory of a recurrent component called the Long-short Term Hybrid Memory (LSTHM). We perform extensive comparisons on six publicly available datasets for multimodal sentiment analysis, speaker trait recognition and emotion recognition. MARN shows state-of-the-art performance on all the datasets.",Multimodal,,Deep Learning,Architectures (Attention),Better accuracy,,,
62,1802.01561,Lasse Espeholt,"Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir
  Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane
  Legg, Koray Kavukcuoglu","IMPALA: Scalable Distributed Deep-RL with Importance Weighted
  Actor-Learner Architectures",,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we aim to solve a large collection of tasks using a single
reinforcement learning agent with a single set of parameters. A key challenge
is to handle the increased amount of data and extended training time. We have
developed a new distributed agent IMPALA (Importance Weighted Actor-Learner
Architecture) that not only uses resources more efficiently in single-machine
training but also scales to thousands of machines without sacrificing data
efficiency or resource utilisation. We achieve stable learning at high
throughput by combining decoupled acting and learning with a novel off-policy
correction method called V-trace. We demonstrate the effectiveness of IMPALA
for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the
DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available
Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our
results show that IMPALA is able to achieve better performance than previous
agents with less data, and crucially exhibits positive transfer between tasks
as a result of its multi-task approach.
","[{'version': 'v1', 'created': 'Mon, 5 Feb 2018 18:47:30 GMT'}, {'version': 'v2', 'created': 'Fri, 9 Feb 2018 15:09:30 GMT'}, {'version': 'v3', 'created': 'Thu, 28 Jun 2018 06:54:39 GMT'}]",2018-06-29,"[['Espeholt', 'Lasse', ''], ['Soyer', 'Hubert', ''], ['Munos', 'Remi', ''], ['Simonyan', 'Karen', ''], ['Mnih', 'Volodymir', ''], ['Ward', 'Tom', ''], ['Doron', 'Yotam', ''], ['Firoiu', 'Vlad', ''], ['Harley', 'Tim', ''], ['Dunning', 'Iain', ''], ['Legg', 'Shane', ''], ['Kavukcuoglu', 'Koray', '']]",2018,6,29,IMPALA: Scalable Distributed Deep-RL with Importance Weighted   Actor-Learner Architectures,60,zscore: 9.611361,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",Games,,Reinforcement Learning,,Better accuracy,,,
63,1802.03480,Martin Simonovsky,"Martin Simonovsky, Nikos Komodakis","GraphVAE: Towards Generation of Small Graphs Using Variational
  Autoencoders",,,,,cs.LG cs.CV cs.NE,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep learning on graphs has become a popular research topic with many
applications. However, past work has concentrated on learning graph embedding
tasks, which is in contrast with advances in generative models for images and
text. Is it possible to transfer this progress to the domain of graphs? We
propose to sidestep hurdles associated with linearization of such discrete
structures by having a decoder output a probabilistic fully-connected graph of
a predefined maximum size directly at once. Our method is formulated as a
variational autoencoder. We evaluate on the challenging task of molecule
generation.
","[{'version': 'v1', 'created': 'Fri, 9 Feb 2018 23:57:46 GMT'}]",2018-02-13,"[['Simonovsky', 'Martin', ''], ['Komodakis', 'Nikos', '']]",2018,2,13,GraphVAE: Towards Generation of Small Graphs Using Variational   Autoencoders,24,zscore: 3.631244,"Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fully-connected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.",Graphs,,Representation Learning,,Better accuracy,,,
64,1802.05296,Yi Zhang,"Sanjeev Arora, Rong Ge, Behnam Neyshabur, Yi Zhang",Stronger generalization bounds for deep nets via a compression approach,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep nets generalize well despite having more parameters than the number of
training samples. Recent works try to give an explanation using PAC-Bayes and
Margin-based analyses, but do not as yet result in sample complexity bounds
better than naive parameter counting. The current paper shows generalization
bounds that're orders of magnitude better in practice. These rely upon new
succinct reparametrizations of the trained net --- a compression that is
explicit and efficient. These yield generalization bounds via a simple
compression-based framework introduced here. Our results also provide some
theoretical justification for widespread empirical success in compressing deep
nets. Analysis of correctness of our compression relies upon some newly
identified \textquotedblleft noise stability\textquotedblright properties of
trained deep nets, which are also experimentally verified. The study of these
properties and resulting generalization bounds are also extended to
convolutional nets, which had eluded earlier attempts on proving
generalization.
","[{'version': 'v1', 'created': 'Wed, 14 Feb 2018 19:38:07 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Feb 2018 17:55:49 GMT'}, {'version': 'v3', 'created': 'Mon, 5 Nov 2018 21:43:00 GMT'}, {'version': 'v4', 'created': 'Mon, 26 Nov 2018 19:31:00 GMT'}]",2018-11-28,"[['Arora', 'Sanjeev', ''], ['Ge', 'Rong', ''], ['Neyshabur', 'Behnam', ''], ['Zhang', 'Yi', '']]",2018,11,28,Stronger generalization bounds for deep nets via a compression approach,36,zscore: 6.077475,"Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet result in sample complexity bounds better than naive parameter counting. The current paper shows generalization bounds that're orders of magnitude better in practice. These rely upon new succinct reparametrizations of the trained net --- a compression that is explicit and efficient. These yield generalization bounds via a simple compression-based framework introduced here. Our results also provide some theoretical justification for widespread empirical success in compressing deep nets. Analysis of correctness of our compression relies upon some newly identified \textquotedblleft noise stability\textquotedblright properties of trained deep nets, which are also experimentally verified. The study of these properties and resulting generalization bounds are also extended to convolutional nets, which had eluded earlier attempts on proving generalization.",Theoretical,,Deep Learning,,Evaluation,,,
65,1802.05666,Jonathan Uesato,"Jonathan Uesato, Brendan O'Donoghue, Aaron van den Oord, Pushmeet
  Kohli",Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,,,,,cs.LG cs.CR stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper investigates recently proposed approaches for defending against
adversarial examples and evaluating adversarial robustness. We motivate
'adversarial risk' as an objective for achieving models robust to worst-case
inputs. We then frame commonly used attacks and evaluation metrics as defining
a tractable surrogate objective to the true adversarial risk. This suggests
that models may optimize this surrogate rather than the true adversarial risk.
We formalize this notion as 'obscurity to an adversary,' and develop tools and
heuristics for identifying obscured models and designing transparent models. We
demonstrate that this is a significant problem in practice by repurposing
gradient-free optimization techniques into adversarial attacks, which we use to
decrease the accuracy of several recently proposed defenses to near zero. Our
hope is that our formulations and results will help researchers to develop more
powerful defenses.
","[{'version': 'v1', 'created': 'Thu, 15 Feb 2018 17:13:18 GMT'}, {'version': 'v2', 'created': 'Tue, 12 Jun 2018 14:20:27 GMT'}]",2018-06-13,"[['Uesato', 'Jonathan', ''], [""O'Donoghue"", 'Brendan', ''], ['Oord', 'Aaron van den', ''], ['Kohli', 'Pushmeet', '']]",2018,6,13,Adversarial Risk and the Dangers of Evaluating Against Weak Attacks,23,zscore: 3.669676,"This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate 'adversarial risk' as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as defining a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as 'obscurity to an adversary,' and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a significant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.",Various,,Adversarial,Examples,Exposing Weaknesses,,,
66,1802.05668,Antonio Polino,"Antonio Polino, Razvan Pascanu, Dan Alistarh",Model compression via distillation and quantization,"21 pages, published as a conference paper at ICLR2018",,,,cs.NE cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks (DNNs) continue to make significant advances, solving
tasks from image classification to translation or reinforcement learning. One
aspect of the field receiving considerable attention is efficiently executing
deep models in resource-constrained environments, such as mobile or embedded
devices. This paper focuses on this problem, and proposes two new compression
methods, which jointly leverage weight quantization and distillation of larger
teacher networks into smaller student networks. The first method we propose is
called quantized distillation and leverages distillation during the training
process, by incorporating distillation loss, expressed with respect to the
teacher, into the training of a student network whose weights are quantized to
a limited set of levels. The second method, differentiable quantization,
optimizes the location of quantization points through stochastic gradient
descent, to better fit the behavior of the teacher model. We validate both
methods through experiments on convolutional and recurrent architectures. We
show that quantized shallow students can reach similar accuracy levels to
full-precision teacher models, while providing order of magnitude compression,
and inference speedup that is linear in the depth reduction. In sum, our
results enable DNNs for resource-constrained environments to leverage
architecture and accuracy advances developed on more powerful devices.
","[{'version': 'v1', 'created': 'Thu, 15 Feb 2018 17:18:49 GMT'}]",2018-02-16,"[['Polino', 'Antonio', ''], ['Pascanu', 'Razvan', ''], ['Alistarh', 'Dan', '']]",2018,2,16,Model compression via distillation and quantization,28,zscore: 4.585260,"Deep neural networks (DNNs) continue to make significant advances, solving tasks from image classification to translation or reinforcement learning. One aspect of the field receiving considerable attention is efficiently executing deep models in resource-constrained environments, such as mobile or embedded devices. This paper focuses on this problem, and proposes two new compression methods, which jointly leverage weight quantization and distillation of larger teacher networks into smaller student networks. The first method we propose is called quantized distillation and leverages distillation during the training process, by incorporating distillation loss, expressed with respect to the teacher, into the training of a student network whose weights are quantized to a limited set of levels. The second method, differentiable quantization, optimizes the location of quantization points through stochastic gradient descent, to better fit the behavior of the teacher model. We validate both methods through experiments on convolutional and recurrent architectures. We show that quantized shallow students can reach similar accuracy levels to full-precision teacher models, while providing order of magnitude compression, and inference speedup that is linear in the depth reduction. In sum, our results enable DNNs for resource-constrained environments to leverage architecture and accuracy advances developed on more powerful devices.",General,Compression,Distillation,,Faster,,,
67,1802.06893,Edouard Grave,"Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, Tomas
  Mikolov",Learning Word Vectors for 157 Languages,Accepted to LREC,,,,cs.CL cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Distributed word representations, or word vectors, have recently been applied
to many tasks in natural language processing, leading to state-of-the-art
performance. A key ingredient to the successful application of these
representations is to train them on very large corpora, and use these
pre-trained models in downstream tasks. In this paper, we describe how we
trained such high quality word representations for 157 languages. We used two
sources of data to train these models: the free online encyclopedia Wikipedia
and data from the common crawl project. We also introduce three new word
analogy datasets to evaluate these word vectors, for French, Hindi and Polish.
Finally, we evaluate our pre-trained word vectors on 10 languages for which
evaluation datasets exists, showing very strong performance compared to
previous models.
","[{'version': 'v1', 'created': 'Mon, 19 Feb 2018 22:32:47 GMT'}, {'version': 'v2', 'created': 'Wed, 28 Mar 2018 18:00:30 GMT'}]",2018-03-30,"[['Grave', 'Edouard', ''], ['Bojanowski', 'Piotr', ''], ['Gupta', 'Prakhar', ''], ['Joulin', 'Armand', ''], ['Mikolov', 'Tomas', '']]",2018,3,30,Learning Word Vectors for 157 Languages,30,zscore: 5.558485,"Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.",Text,,Representation Learning,,Better accuracy,,,
68,1802.07687,Emily Denton,Emily Denton and Rob Fergus,Stochastic Video Generation with a Learned Prior,,,,,cs.CV cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Generating video frames that accurately predict future world states is
challenging. Existing approaches either fail to capture the full distribution
of outcomes, or yield blurry generations, or both. In this paper we introduce
an unsupervised video generation model that learns a prior model of uncertainty
in a given environment. Video frames are generated by drawing samples from this
prior and combining them with a deterministic estimate of the future frame. The
approach is simple and easily trained end-to-end on a variety of datasets.
Sample generations are both varied and sharp, even many frames into the future,
and compare favorably to those from existing approaches.
","[{'version': 'v1', 'created': 'Wed, 21 Feb 2018 17:36:27 GMT'}, {'version': 'v2', 'created': 'Fri, 2 Mar 2018 17:39:23 GMT'}]",2018-03-05,"[['Denton', 'Emily', ''], ['Fergus', 'Rob', '']]",2018,3,5,Stochastic Video Generation with a Learned Prior,29,zscore: 5.112426,"Generating video frames that accurately predict future world states is challenging. Existing approaches either fail to capture the full distribution of outcomes, or yield blurry generations, or both. In this paper we introduce an unsupervised video generation model that learns a prior model of uncertainty in a given environment. Video frames are generated by drawing samples from this prior and combining them with a deterministic estimate of the future frame. The approach is simple and easily trained end-to-end on a variety of datasets. Sample generations are both varied and sharp, even many frames into the future, and compare favorably to those from existing approaches.",Video,,Generation,Video,Better accuracy,,,
69,1802.08908,Nicolas Papernot,"Nicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal
  Talwar, \'Ulfar Erlingsson",Scalable Private Learning with PATE,Published as a conference paper at ICLR 2018,,,,stat.ML cs.CR cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  The rapid adoption of machine learning has increased concerns about the
privacy implications of machine learning models trained on sensitive data, such
as medical records or other personal information. To address those concerns,
one promising approach is Private Aggregation of Teacher Ensembles, or PATE,
which transfers to a ""student"" model the knowledge of an ensemble of ""teacher""
models, with intuitive privacy provided by training teachers on disjoint data
and strong privacy guaranteed by noisy aggregation of teachers' answers.
However, PATE has so far been evaluated only on simple classification tasks
like MNIST, leaving unclear its utility when applied to larger-scale learning
tasks and real-world datasets.
  In this work, we show how PATE can scale to learning tasks with large numbers
of output classes and uncurated, imbalanced training data with errors. For
this, we introduce new noisy aggregation mechanisms for teacher ensembles that
are more selective and add less noise, and prove their tighter
differential-privacy guarantees. Our new mechanisms build on two insights: the
chance of teacher consensus is increased by using more concentrated noise and,
lacking consensus, no answer need be given to a student. The consensus answers
used are more likely to be correct, offer better intuitive privacy, and incur
lower-differential privacy cost. Our evaluation shows our mechanisms improve on
the original PATE on all measures, and scale to larger tasks with both high
utility and very strong privacy ($\varepsilon$ < 1.0).
","[{'version': 'v1', 'created': 'Sat, 24 Feb 2018 20:39:51 GMT'}]",2018-02-27,"[['Papernot', 'Nicolas', ''], ['Song', 'Shuang', ''], ['Mironov', 'Ilya', ''], ['Raghunathan', 'Ananth', ''], ['Talwar', 'Kunal', ''], ['Erlingsson', 'Úlfar', '']]",2018,2,27,Scalable Private Learning with PATE,26,zscore: 4.404580,"The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ""student"" model the knowledge of an ensemble of ""teacher"" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets.   In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy ($\varepsilon$ < 1.0).",Various,Privacy,Learning,Ensembles,Better accuracy,,,
70,1802.09081,Vitchyr H. Pong,"Vitchyr Pong, Shixiang Gu, Murtaza Dalal, Sergey Levine",Temporal Difference Models: Model-Free Deep RL for Model-Based Control,Appeared in ICLR 2018; typos corrected,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Model-free reinforcement learning (RL) is a powerful, general tool for
learning complex behaviors. However, its sample efficiency is often
impractically large for solving challenging real-world problems, even with
off-policy algorithms such as Q-learning. A limiting factor in classic
model-free RL is that the learning signal consists only of scalar rewards,
ignoring much of the rich information contained in state transition tuples.
Model-based RL uses this information, by training a predictive model, but often
does not achieve the same asymptotic performance as model-free RL due to model
bias. We introduce temporal difference models (TDMs), a family of
goal-conditioned value functions that can be trained with model-free learning
and used for model-based control. TDMs combine the benefits of model-free and
model-based RL: they leverage the rich information in state transitions to
learn very efficiently, while still attaining asymptotic performance that
exceeds that of direct model-based RL methods. Our experimental results show
that, on a range of continuous control tasks, TDMs provide a substantial
improvement in efficiency compared to state-of-the-art model-based and
model-free methods.
","[{'version': 'v1', 'created': 'Sun, 25 Feb 2018 21:14:44 GMT'}, {'version': 'v2', 'created': 'Mon, 24 Feb 2020 06:34:11 GMT'}]",2020-02-25,"[['Pong', 'Vitchyr', ''], ['Gu', 'Shixiang', ''], ['Dalal', 'Murtaza', ''], ['Levine', 'Sergey', '']]",2020,2,25,Temporal Difference Models: Model-Free Deep RL for Model-Based Control,23,zscore: 3.909724,"Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods.",Various,,Reinforcement Learning,,Better accuracy,,,
71,1803.00933,Daniel Horgan,"Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo
  Hessel, Hado van Hasselt and David Silver",Distributed Prioritized Experience Replay,Accepted to International Conference on Learning Representations 2018,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We propose a distributed architecture for deep reinforcement learning at
scale, that enables agents to learn effectively from orders of magnitude more
data than previously possible. The algorithm decouples acting from learning:
the actors interact with their own instances of the environment by selecting
actions according to a shared neural network, and accumulate the resulting
experience in a shared experience replay memory; the learner replays samples of
experience and updates the neural network. The architecture relies on
prioritized experience replay to focus only on the most significant data
generated by the actors. Our architecture substantially improves the state of
the art on the Arcade Learning Environment, achieving better final performance
in a fraction of the wall-clock training time.
","[{'version': 'v1', 'created': 'Fri, 2 Mar 2018 16:21:46 GMT'}]",2018-03-05,"[['Horgan', 'Dan', ''], ['Quan', 'John', ''], ['Budden', 'David', ''], ['Barth-Maron', 'Gabriel', ''], ['Hessel', 'Matteo', ''], ['van Hasselt', 'Hado', ''], ['Silver', 'David', '']]",2018,3,5,Distributed Prioritized Experience Replay,28,zscore: 5.179243,"We propose a distributed architecture for deep reinforcement learning at scale, that enables agents to learn effectively from orders of magnitude more data than previously possible. The algorithm decouples acting from learning: the actors interact with their own instances of the environment by selecting actions according to a shared neural network, and accumulate the resulting experience in a shared experience replay memory; the learner replays samples of experience and updates the neural network. The architecture relies on prioritized experience replay to focus only on the most significant data generated by the actors. Our architecture substantially improves the state of the art on the Arcade Learning Environment, achieving better final performance in a fraction of the wall-clock training time.",Games,,Reinforcement Learning,,Faster,,,
72,1803.01206,Simon Du,Simon S. Du and Jason D. Lee,"On the Power of Over-parametrization in Neural Networks with Quadratic
  Activation",Accepted by ICML 2018,,,,cs.LG cs.AI math.OC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We provide new theoretical insights on why over-parametrization is effective
in learning neural networks. For a $k$ hidden node shallow network with
quadratic activation and $n$ training data points, we show as long as $ k \ge
\sqrt{2n}$, over-parametrization enables local search algorithms to find a
\emph{globally} optimal solution for general smooth and convex loss functions.
Further, despite that the number of parameters may exceed the sample size,
using theory of Rademacher complexity, we show with weight decay, the solution
also generalizes well if the data is sampled from a regular distribution such
as Gaussian. To prove when $k\ge \sqrt{2n}$, the loss function has benign
landscape properties, we adopt an idea from smoothed analysis, which may have
other applications in studying loss surfaces of neural networks.
","[{'version': 'v1', 'created': 'Sat, 3 Mar 2018 17:37:57 GMT'}, {'version': 'v2', 'created': 'Thu, 14 Jun 2018 23:59:37 GMT'}]",2018-06-18,"[['Du', 'Simon S.', ''], ['Lee', 'Jason D.', '']]",2018,6,18,On the Power of Over-parametrization in Neural Networks with Quadratic   Activation,28,zscore: 5.061470,"We provide new theoretical insights on why over-parametrization is effective in learning neural networks. For a $k$ hidden node shallow network with quadratic activation and $n$ training data points, we show as long as $ k \ge \sqrt{2n}$, over-parametrization enables local search algorithms to find a \emph{globally} optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when $k\ge \sqrt{2n}$, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks.",Theoretical,,Learning,Aspects of DL,Evaluation,,,
73,1803.01442,Guneet Dhillon,"Guneet S. Dhillon, Kamyar Azizzadenesheli, Zachary C. Lipton, Jeremy
  Bernstein, Jean Kossaifi, Aran Khanna, Anima Anandkumar",Stochastic Activation Pruning for Robust Adversarial Defense,ICLR 2018,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Neural networks are known to be vulnerable to adversarial examples. Carefully
chosen perturbations to real images, while imperceptible to humans, induce
misclassification and threaten the reliability of deep learning systems in the
wild. To guard against adversarial examples, we take inspiration from game
theory and cast the problem as a minimax zero-sum game between the adversary
and the model. In general, for such games, the optimal strategy for both
players requires a stochastic policy, also known as a mixed strategy. In this
light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for
adversarial defense. SAP prunes a random subset of activations (preferentially
pruning those with smaller magnitude) and scales up the survivors to
compensate. We can apply SAP to pretrained networks, including adversarially
trained models, without fine-tuning, providing robustness against adversarial
examples. Experiments demonstrate that SAP confers robustness against attacks,
increasing accuracy and preserving calibration.
","[{'version': 'v1', 'created': 'Mon, 5 Mar 2018 00:17:05 GMT'}]",2018-03-06,"[['Dhillon', 'Guneet S.', ''], ['Azizzadenesheli', 'Kamyar', ''], ['Lipton', 'Zachary C.', ''], ['Bernstein', 'Jeremy', ''], ['Kossaifi', 'Jean', ''], ['Khanna', 'Aran', ''], ['Anandkumar', 'Anima', '']]",2018,3,6,Stochastic Activation Pruning for Robust Adversarial Defense,33,zscore: 6.202125,"Neural networks are known to be vulnerable to adversarial examples. Carefully chosen perturbations to real images, while imperceptible to humans, induce misclassification and threaten the reliability of deep learning systems in the wild. To guard against adversarial examples, we take inspiration from game theory and cast the problem as a minimax zero-sum game between the adversary and the model. In general, for such games, the optimal strategy for both players requires a stochastic policy, also known as a mixed strategy. In this light, we propose Stochastic Activation Pruning (SAP), a mixed strategy for adversarial defense. SAP prunes a random subset of activations (preferentially pruning those with smaller magnitude) and scales up the survivors to compensate. We can apply SAP to pretrained networks, including adversarially trained models, without fine-tuning, providing robustness against adversarial examples. Experiments demonstrate that SAP confers robustness against attacks, increasing accuracy and preserving calibration.",Image ,,Adversarial,Attacks,Better accuracy,,,
74,1803.02893,Lajanugen Logeswaran,"Lajanugen Logeswaran, Honglak Lee",An efficient framework for learning sentence representations,ICLR 2018,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work we propose a simple and efficient framework for learning
sentence representations from unlabelled data. Drawing inspiration from the
distributional hypothesis and recent work on learning sentence representations,
we reformulate the problem of predicting the context in which a sentence
appears as a classification problem. Given a sentence and its context, a
classifier distinguishes context sentences from other contrastive sentences
based on their vector representations. This allows us to efficiently learn
different types of encoding functions, and we show that the model learns
high-quality sentence representations. We demonstrate that our sentence
representations outperform state-of-the-art unsupervised and supervised
representation learning methods on several downstream NLP tasks that involve
understanding sentence semantics while achieving an order of magnitude speedup
in training time.
","[{'version': 'v1', 'created': 'Wed, 7 Mar 2018 22:02:10 GMT'}]",2018-03-09,"[['Logeswaran', 'Lajanugen', ''], ['Lee', 'Honglak', '']]",2018,3,9,An efficient framework for learning sentence representations,25,zscore: 4.647373,"In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.",Text,,Representation Learning,,Better accuracy,,,
75,1803.02999,John Schulman,"Alex Nichol, Joshua Achiam, John Schulman",On First-Order Meta-Learning Algorithms,,,,,cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This paper considers meta-learning problems, where there is a distribution of
tasks, and we would like to obtain an agent that performs well (i.e., learns
quickly) when presented with a previously unseen task sampled from this
distribution. We analyze a family of algorithms for learning a parameter
initialization that can be fine-tuned quickly on a new task, using only
first-order derivatives for the meta-learning updates. This family includes and
generalizes first-order MAML, an approximation to MAML obtained by ignoring
second-order derivatives. It also includes Reptile, a new algorithm that we
introduce here, which works by repeatedly sampling a task, training on it, and
moving the initialization towards the trained weights on that task. We expand
on the results from Finn et al. showing that first-order meta-learning
algorithms perform well on some well-established benchmarks for few-shot
classification, and we provide theoretical analysis aimed at understanding why
these algorithms work.
","[{'version': 'v1', 'created': 'Thu, 8 Mar 2018 08:29:38 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Apr 2018 19:00:28 GMT'}, {'version': 'v3', 'created': 'Mon, 22 Oct 2018 16:11:14 GMT'}]",2018-10-23,"[['Nichol', 'Alex', ''], ['Achiam', 'Joshua', ''], ['Schulman', 'John', '']]",2018,10,23,On First-Order Meta-Learning Algorithms,22,zscore: 4.035805,"This paper considers meta-learning problems, where there is a distribution of tasks, and we would like to obtain an agent that performs well (i.e., learns quickly) when presented with a previously unseen task sampled from this distribution. We analyze a family of algorithms for learning a parameter initialization that can be fine-tuned quickly on a new task, using only first-order derivatives for the meta-learning updates. This family includes and generalizes first-order MAML, an approximation to MAML obtained by ignoring second-order derivatives. It also includes Reptile, a new algorithm that we introduce here, which works by repeatedly sampling a task, training on it, and moving the initialization towards the trained weights on that task. We expand on the results from Finn et al. showing that first-order meta-learning algorithms perform well on some well-established benchmarks for few-shot classification, and we provide theoretical analysis aimed at understanding why these algorithms work.",Various,,Learning,Meta-Learning,Evaluation,,,
76,1803.04383,Lydia T. Liu,"Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, Moritz Hardt",Delayed Impact of Fair Machine Learning,"37 pages, 6 figures","Proceedings of the 35th International Conference on Machine
  Learning, PMLR 80:3150-3158, 2018",,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Fairness in machine learning has predominantly been studied in static
classification settings without concern for how decisions change the underlying
population over time. Conventional wisdom suggests that fairness criteria
promote the long-term well-being of those groups they aim to protect.
  We study how static fairness criteria interact with temporal indicators of
well-being, such as long-term improvement, stagnation, and decline in a
variable of interest. We demonstrate that even in a one-step feedback model,
common fairness criteria in general do not promote improvement over time, and
may in fact cause harm in cases where an unconstrained objective would not.
  We completely characterize the delayed impact of three standard criteria,
contrasting the regimes in which these exhibit qualitatively different
behavior. In addition, we find that a natural form of measurement error
broadens the regime in which fairness criteria perform favorably.
  Our results highlight the importance of measurement and temporal modeling in
the evaluation of fairness criteria, suggesting a range of new challenges and
trade-offs.
","[{'version': 'v1', 'created': 'Mon, 12 Mar 2018 17:20:56 GMT'}, {'version': 'v2', 'created': 'Sat, 7 Apr 2018 20:34:16 GMT'}]",2018-08-10,"[['Liu', 'Lydia T.', ''], ['Dean', 'Sarah', ''], ['Rolf', 'Esther', ''], ['Simchowitz', 'Max', ''], ['Hardt', 'Moritz', '']]",2018,8,10,Delayed Impact of Fair Machine Learning,21,zscore: 3.764111,"Fairness in machine learning has predominantly been studied in static classification settings without concern for how decisions change the underlying population over time. Conventional wisdom suggests that fairness criteria promote the long-term well-being of those groups they aim to protect.   We study how static fairness criteria interact with temporal indicators of well-being, such as long-term improvement, stagnation, and decline in a variable of interest. We demonstrate that even in a one-step feedback model, common fairness criteria in general do not promote improvement over time, and may in fact cause harm in cases where an unconstrained objective would not.   We completely characterize the delayed impact of three standard criteria, contrasting the regimes in which these exhibit qualitatively different behavior. In addition, we find that a natural form of measurement error broadens the regime in which fairness criteria perform favorably.   Our results highlight the importance of measurement and temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",Other,Fairness in ML,Analysis,,Exposing Weaknesses,,,of researchers
77,1803.04765,Nicolas Papernot,Nicolas Papernot and Patrick McDaniel,"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust
  Deep Learning",,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Deep neural networks (DNNs) enable innovative applications of machine
learning like image recognition, machine translation, or malware detection.
However, deep learning is often criticized for its lack of robustness in
adversarial settings (e.g., vulnerability to adversarial inputs) and general
inability to rationalize its predictions. In this work, we exploit the
structure of deep learning to enable new learning-based inference and decision
strategies that achieve desirable properties such as robustness and
interpretability. We take a first step in this direction and introduce the Deep
k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest
neighbors algorithm with representations of the data learned by each layer of
the DNN: a test input is compared to its neighboring training points according
to the distance that separates them in the representations. We show the labels
of these neighboring points afford confidence estimates for inputs outside the
model's training manifold, including on malicious inputs like adversarial
examples--and therein provides protections against inputs that are outside the
models understanding. This is because the nearest neighbors can be used to
estimate the nonconformity of, i.e., the lack of support for, a prediction in
the training data. The neighbors also constitute human-interpretable
explanations of predictions. We evaluate the DkNN algorithm on several
datasets, and show the confidence estimates accurately identify inputs outside
the model, and that the explanations provided by nearest neighbors are
intuitive and useful in understanding model failures.
","[{'version': 'v1', 'created': 'Tue, 13 Mar 2018 13:02:13 GMT'}]",2018-03-14,"[['Papernot', 'Nicolas', ''], ['McDaniel', 'Patrick', '']]",2018,3,14,"Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust   Deep Learning",20,zscore: 3.648149,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.",Various,,Interpretability,,Robustness,,,
78,1803.06373,Harini Kannan,"Harini Kannan, Alexey Kurakin, Ian Goodfellow",Adversarial Logit Pairing,10 pages,,,,cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this paper, we develop improved techniques for defending against
adversarial examples at scale. First, we implement the state of the art version
of adversarial training at unprecedented scale on ImageNet and investigate
whether it remains effective in this setting - an important open scientific
question (Athalye et al., 2018). Next, we introduce enhanced defenses using a
technique we call logit pairing, a method that encourages logits for pairs of
examples to be similar. When applied to clean examples and their adversarial
counterparts, logit pairing improves accuracy on adversarial examples over
vanilla adversarial training; we also find that logit pairing on clean examples
only is competitive with adversarial training in terms of accuracy on two
datasets. Finally, we show that adversarial logit pairing achieves the state of
the art defense on ImageNet against PGD white box attacks, with an accuracy
improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully
damages the current state of the art defense against black box attacks on
ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With
this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018)
for the state of the art on black box attacks on ImageNet.
","[{'version': 'v1', 'created': 'Fri, 16 Mar 2018 19:03:45 GMT'}]",2018-03-20,"[['Kannan', 'Harini', ''], ['Kurakin', 'Alexey', ''], ['Goodfellow', 'Ian', '']]",2018,3,20,Adversarial Logit Pairing,19,zscore: 3.939077,"In this paper, we develop improved techniques for defending against adversarial examples at scale. First, we implement the state of the art version of adversarial training at unprecedented scale on ImageNet and investigate whether it remains effective in this setting - an important open scientific question (Athalye et al., 2018). Next, we introduce enhanced defenses using a technique we call logit pairing, a method that encourages logits for pairs of examples to be similar. When applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets. Finally, we show that adversarial logit pairing achieves the state of the art defense on ImageNet against PGD white box attacks, with an accuracy improvement from 1.5% to 27.9%. Adversarial logit pairing also successfully damages the current state of the art defense against black box attacks on ImageNet (Tramer et al., 2018), dropping its accuracy from 66.6% to 47.1%. With this new accuracy drop, adversarial logit pairing ties with Tramer et al.(2018) for the state of the art on black box attacks on ImageNet.",Image,,Adversarial,Examples,Exposing Weaknesses,Shielding,,
79,1803.07055,Horia Mania,"Horia Mania, Aurelia Guy, Benjamin Recht","Simple random search provides a competitive approach to reinforcement
  learning","22 pages, 5 figures, 9 tables",,,,cs.LG cs.AI math.OC stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A common belief in model-free reinforcement learning is that methods based on
random search in the parameter space of policies exhibit significantly worse
sample complexity than those that explore the space of actions. We dispel such
beliefs by introducing a random search method for training static, linear
policies for continuous control problems, matching state-of-the-art sample
efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a
nearly optimal controller for a challenging instance of the Linear Quadratic
Regulator, a classical problem in control theory, when the dynamics are not
known. Computationally, our random search algorithm is at least 15 times more
efficient than the fastest competing model-free methods on these benchmarks. We
take advantage of this computational efficiency to evaluate the performance of
our method over hundreds of random seeds and many different hyperparameter
configurations for each benchmark task. Our simulations highlight a high
variability in performance in these benchmark tasks, suggesting that commonly
used estimations of sample efficiency do not adequately evaluate the
performance of RL algorithms.
","[{'version': 'v1', 'created': 'Mon, 19 Mar 2018 17:35:14 GMT'}]",2018-03-20,"[['Mania', 'Horia', ''], ['Guy', 'Aurelia', ''], ['Recht', 'Benjamin', '']]",2018,3,20,Simple random search provides a competitive approach to reinforcement   learning,34,zscore: 8.019439,"A common belief in model-free reinforcement learning is that methods based on random search in the parameter space of policies exhibit significantly worse sample complexity than those that explore the space of actions. We dispel such beliefs by introducing a random search method for training static, linear policies for continuous control problems, matching state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Our method also finds a nearly optimal controller for a challenging instance of the Linear Quadratic Regulator, a classical problem in control theory, when the dynamics are not known. Computationally, our random search algorithm is at least 15 times more efficient than the fastest competing model-free methods on these benchmarks. We take advantage of this computational efficiency to evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. Our simulations highlight a high variability in performance in these benchmark tasks, suggesting that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms.",Locomotion,,Reinforcement Learning,,Faster,,,
80,1803.08494,Kaiming He,"Yuxin Wu, Kaiming He",Group Normalization,"v3: Update trained-from-scratch results in COCO to 41.0AP. Code and
  models at
  https://github.com/facebookresearch/Detectron/blob/master/projects/GN",,,,cs.CV cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Batch Normalization (BN) is a milestone technique in the development of deep
learning, enabling various networks to train. However, normalizing along the
batch dimension introduces problems --- BN's error increases rapidly when the
batch size becomes smaller, caused by inaccurate batch statistics estimation.
This limits BN's usage for training larger models and transferring features to
computer vision tasks including detection, segmentation, and video, which
require small batches constrained by memory consumption. In this paper, we
present Group Normalization (GN) as a simple alternative to BN. GN divides the
channels into groups and computes within each group the mean and variance for
normalization. GN's computation is independent of batch sizes, and its accuracy
is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN
has 10.6% lower error than its BN counterpart when using a batch size of 2;
when using typical batch sizes, GN is comparably good with BN and outperforms
other normalization variants. Moreover, GN can be naturally transferred from
pre-training to fine-tuning. GN can outperform its BN-based counterparts for
object detection and segmentation in COCO, and for video classification in
Kinetics, showing that GN can effectively replace the powerful BN in a variety
of tasks. GN can be easily implemented by a few lines of code in modern
libraries.
","[{'version': 'v1', 'created': 'Thu, 22 Mar 2018 17:57:16 GMT'}, {'version': 'v2', 'created': 'Tue, 24 Apr 2018 18:12:19 GMT'}, {'version': 'v3', 'created': 'Mon, 11 Jun 2018 22:48:02 GMT'}]",2018-06-13,"[['Wu', 'Yuxin', ''], ['He', 'Kaiming', '']]",2018,6,13,Group Normalization,24,zscore: 5.392345,"Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.",Image ,,Learning,Aspects of DL,Better accuracy,,,
81,1803.09017,Yuxuan Wang,"Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ Skerry-Ryan, Eric Battenberg,
  Joel Shor, Ying Xiao, Fei Ren, Ye Jia, Rif A. Saurous","Style Tokens: Unsupervised Style Modeling, Control and Transfer in
  End-to-End Speech Synthesis",,,,,cs.CL cs.LG cs.SD eess.AS,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In this work, we propose ""global style tokens"" (GSTs), a bank of embeddings
that are jointly trained within Tacotron, a state-of-the-art end-to-end speech
synthesis system. The embeddings are trained with no explicit labels, yet learn
to model a large range of acoustic expressiveness. GSTs lead to a rich set of
significant results. The soft interpretable ""labels"" they generate can be used
to control synthesis in novel ways, such as varying speed and speaking style -
independently of the text content. They can also be used for style transfer,
replicating the speaking style of a single audio clip across an entire
long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn
to factorize noise and speaker identity, providing a path towards highly
scalable but robust speech synthesis.
","[{'version': 'v1', 'created': 'Fri, 23 Mar 2018 23:56:49 GMT'}]",2018-03-28,"[['Wang', 'Yuxuan', ''], ['Stanton', 'Daisy', ''], ['Zhang', 'Yu', ''], ['Skerry-Ryan', 'RJ', ''], ['Battenberg', 'Eric', ''], ['Shor', 'Joel', ''], ['Xiao', 'Ying', ''], ['Ren', 'Fei', ''], ['Jia', 'Ye', ''], ['Saurous', 'Rif A.', '']]",2018,3,28,"Style Tokens: Unsupervised Style Modeling, Control and Transfer in   End-to-End Speech Synthesis",20,zscore: 4.419542,"In this work, we propose ""global style tokens"" (GSTs), a bank of embeddings that are jointly trained within Tacotron, a state-of-the-art end-to-end speech synthesis system. The embeddings are trained with no explicit labels, yet learn to model a large range of acoustic expressiveness. GSTs lead to a rich set of significant results. The soft interpretable ""labels"" they generate can be used to control synthesis in novel ways, such as varying speed and speaking style - independently of the text content. They can also be used for style transfer, replicating the speaking style of a single audio clip across an entire long-form text corpus. When trained on noisy, unlabeled found data, GSTs learn to factorize noise and speaker identity, providing a path towards highly scalable but robust speech synthesis.",Speech,Synthesis,Generation,"Style transfer, Seq2Seq",Better accuracy,,,
82,1804.00645,Aravind Srinivas,"Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, Chelsea
  Finn",Universal Planning Networks,Videos available at https://sites.google.com/view/upn-public/home,,,,cs.LG cs.AI cs.CV cs.RO stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A key challenge in complex visuomotor control is learning abstract
representations that are effective for specifying goals, planning, and
generalization. To this end, we introduce universal planning networks (UPN).
UPNs embed differentiable planning within a goal-directed policy. This planning
computation unrolls a forward model in a latent space and infers an optimal
action plan through gradient descent trajectory optimization. The
plan-by-gradient-descent process and its underlying representations are learned
end-to-end to directly optimize a supervised imitation learning objective. We
find that the representations learned are not only effective for goal-directed
visual imitation via gradient-based trajectory optimization, but can also
provide a metric for specifying goals using images. The learned representations
can be leveraged to specify distance-based rewards to reach new target states
for model-free reinforcement learning, resulting in substantially more
effective learning when solving new tasks described via image-based goals. We
were able to achieve successful transfer of visuomotor planning strategies
across robots with significantly different morphologies and actuation
capabilities.
","[{'version': 'v1', 'created': 'Mon, 2 Apr 2018 17:51:53 GMT'}, {'version': 'v2', 'created': 'Wed, 4 Apr 2018 17:36:36 GMT'}]",2018-04-05,"[['Srinivas', 'Aravind', ''], ['Jabri', 'Allan', ''], ['Abbeel', 'Pieter', ''], ['Levine', 'Sergey', ''], ['Finn', 'Chelsea', '']]",2018,4,5,Universal Planning Networks,17,zscore: 3.875451,"A key challenge in complex visuomotor control is learning abstract representations that are effective for specifying goals, planning, and generalization. To this end, we introduce universal planning networks (UPN). UPNs embed differentiable planning within a goal-directed policy. This planning computation unrolls a forward model in a latent space and infers an optimal action plan through gradient descent trajectory optimization. The plan-by-gradient-descent process and its underlying representations are learned end-to-end to directly optimize a supervised imitation learning objective. We find that the representations learned are not only effective for goal-directed visual imitation via gradient-based trajectory optimization, but can also provide a metric for specifying goals using images. The learned representations can be leveraged to specify distance-based rewards to reach new target states for model-free reinforcement learning, resulting in substantially more effective learning when solving new tasks described via image-based goals. We were able to achieve successful transfer of visuomotor planning strategies across robots with significantly different morphologies and actuation capabilities.",Locomotion,,Reinforcement Learning,,Better accuracy,,,
83,1804.01523,Alex Lee,"Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea
  Finn, Sergey Levine",Stochastic Adversarial Video Prediction,Website: https://alexlee-gk.github.io/video_prediction/,,,,cs.CV cs.AI cs.LG cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Being able to predict what may happen in the future requires an in-depth
understanding of the physical and causal rules that govern the world. A model
that is able to do so has a number of appealing applications, from robotic
planning to representation learning. However, learning to predict raw future
observations, such as frames in a video, is exceedingly challenging -- the
ambiguous nature of the problem can cause a naively designed model to average
together possible futures into a single, blurry prediction. Recently, this has
been addressed by two distinct approaches: (a) latent variational variable
models that explicitly model underlying stochasticity and (b)
adversarially-trained models that aim to produce naturalistic images. However,
a standard latent variable model can struggle to produce realistic results, and
a standard adversarially-trained model underutilizes latent variables and fails
to produce diverse predictions. We show that these distinct methods are in fact
complementary. Combining the two produces predictions that look more realistic
to human raters and better cover the range of possible futures. Our method
outperforms prior and concurrent work in these aspects.
","[{'version': 'v1', 'created': 'Wed, 4 Apr 2018 17:55:40 GMT'}]",2018-04-05,"[['Lee', 'Alex X.', ''], ['Zhang', 'Richard', ''], ['Ebert', 'Frederik', ''], ['Abbeel', 'Pieter', ''], ['Finn', 'Chelsea', ''], ['Levine', 'Sergey', '']]",2018,4,5,Stochastic Adversarial Video Prediction,19,zscore: 4.647139,"Being able to predict what may happen in the future requires an in-depth understanding of the physical and causal rules that govern the world. A model that is able to do so has a number of appealing applications, from robotic planning to representation learning. However, learning to predict raw future observations, such as frames in a video, is exceedingly challenging -- the ambiguous nature of the problem can cause a naively designed model to average together possible futures into a single, blurry prediction. Recently, this has been addressed by two distinct approaches: (a) latent variational variable models that explicitly model underlying stochasticity and (b) adversarially-trained models that aim to produce naturalistic images. However, a standard latent variable model can struggle to produce realistic results, and a standard adversarially-trained model underutilizes latent variables and fails to produce diverse predictions. We show that these distinct methods are in fact complementary. Combining the two produces predictions that look more realistic to human raters and better cover the range of possible futures. Our method outperforms prior and concurrent work in these aspects.",Video,,GAN,,Better accuracy,,,
84,1804.02717,Xue Bin Peng,"Xue Bin Peng, Pieter Abbeel, Sergey Levine, Michiel van de Panne","DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based
  Character Skills",,,10.1145/3197517.3201311,,cs.GR cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  A longstanding goal in character animation is to combine data-driven
specification of behavior with a system that can execute a similar behavior in
a physical simulation, thus enabling realistic responses to perturbations and
environmental variation. We show that well-known reinforcement learning (RL)
methods can be adapted to learn robust control policies capable of imitating a
broad range of example motion clips, while also learning complex recoveries,
adapting to changes in morphology, and accomplishing user-specified goals. Our
method handles keyframed motions, highly-dynamic actions such as
motion-captured flips and spins, and retargeted motions. By combining a
motion-imitation objective with a task objective, we can train characters that
react intelligently in interactive settings, e.g., by walking in a desired
direction or throwing a ball at a user-specified target. This approach thus
combines the convenience and motion quality of using motion clips to define the
desired style and appearance, with the flexibility and generality afforded by
RL methods and physics-based animation. We further explore a number of methods
for integrating multiple clips into the learning process to develop
multi-skilled agents capable of performing a rich repertoire of diverse skills.
We demonstrate results using multiple characters (human, Atlas robot, bipedal
dinosaur, dragon) and a large variety of skills, including locomotion,
acrobatics, and martial arts.
","[{'version': 'v1', 'created': 'Sun, 8 Apr 2018 17:04:58 GMT'}, {'version': 'v2', 'created': 'Sat, 16 Jun 2018 20:48:52 GMT'}, {'version': 'v3', 'created': 'Fri, 27 Jul 2018 03:44:10 GMT'}]",2018-08-07,"[['Peng', 'Xue Bin', ''], ['Abbeel', 'Pieter', ''], ['Levine', 'Sergey', ''], ['van de Panne', 'Michiel', '']]",2018,8,7,DeepMimic: Example-Guided Deep Reinforcement Learning of Physics-Based   Character Skills,27,zscore: 6.640216,"A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.",Locomotion,,Reinforcement Learning,,Better accuracy,,,
85,1804.03599,Christopher Burgess,"Christopher P. Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick
  Watters, Guillaume Desjardins, Alexander Lerchner",Understanding disentangling in $\beta$-VAE,"Presented at the 2017 NIPS Workshop on Learning Disentangled
  Representations",,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present new intuitions and theoretical assessments of the emergence of
disentangled representation in variational autoencoders. Taking a
rate-distortion theory perspective, we show the circumstances under which
representations aligned with the underlying generative factors of variation of
data emerge when optimising the modified ELBO bound in $\beta$-VAE, as training
progresses. From these insights, we propose a modification to the training
regime of $\beta$-VAE, that progressively increases the information capacity of
the latent code during training. This modification facilitates the robust
learning of disentangled representations in $\beta$-VAE, without the previous
trade-off in reconstruction accuracy.
","[{'version': 'v1', 'created': 'Tue, 10 Apr 2018 15:48:18 GMT'}]",2018-04-11,"[['Burgess', 'Christopher P.', ''], ['Higgins', 'Irina', ''], ['Pal', 'Arka', ''], ['Matthey', 'Loic', ''], ['Watters', 'Nick', ''], ['Desjardins', 'Guillaume', ''], ['Lerchner', 'Alexander', '']]",2018,4,11,Understanding disentangling in $\beta$-VAE,18,zscore: 4.184081,"We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in $\beta$-VAE, as training progresses. From these insights, we propose a modification to the training regime of $\beta$-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in $\beta$-VAE, without the previous trade-off in reconstruction accuracy.",Theoretical,,Deep Learning,Architectures (Autoencoders),Robustness,,,
86,1804.06561,Song Mei,"Song Mei, Andrea Montanari, Phan-Minh Nguyen",A Mean Field View of the Landscape of Two-Layers Neural Networks,103 pages,,,,stat.ML cond-mat.stat-mech cs.LG math.ST stat.TH,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Multi-layer neural networks are among the most powerful models in machine
learning, yet the fundamental reasons for this success defy mathematical
understanding. Learning a neural network requires to optimize a non-convex
high-dimensional objective (risk function), a problem which is usually attacked
using stochastic gradient descent (SGD). Does SGD converge to a global optimum
of the risk or only to a local optimum? In the first case, does this happen
because local minima are absent, or because SGD somehow avoids them? In the
second, why do local minima reached by SGD have good generalization properties?
  In this paper we consider a simple case, namely two-layers neural networks,
and prove that -in a suitable scaling limit- SGD dynamics is captured by a
certain non-linear partial differential equation (PDE) that we call
distributional dynamics (DD). We then consider several specific examples, and
show how DD can be used to prove convergence of SGD to networks with nearly
ideal generalization error. This description allows to 'average-out' some of
the complexities of the landscape of neural networks, and can be used to prove
a general convergence result for noisy SGD.
","[{'version': 'v1', 'created': 'Wed, 18 Apr 2018 05:31:45 GMT'}, {'version': 'v2', 'created': 'Tue, 28 Aug 2018 06:21:23 GMT'}]",2018-08-29,"[['Mei', 'Song', ''], ['Montanari', 'Andrea', ''], ['Nguyen', 'Phan-Minh', '']]",2018,8,29,A Mean Field View of the Landscape of Two-Layers Neural Networks,19,zscore: 3.803656,"Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties?   In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.",Theoretical,Apects of DL,Analysis, ,Convergence,,,
87,1804.07612,Dominic Masters,Dominic Masters and Carlo Luschi,Revisiting Small Batch Training for Deep Neural Networks,,,,,cs.LG cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Modern deep neural network training is typically based on mini-batch
stochastic gradient optimization. While the use of large mini-batches increases
the available computational parallelism, small batch training has been shown to
provide improved generalization performance and allows a significantly smaller
memory footprint, which might also be exploited to improve machine throughput.
  In this paper, we review common assumptions on learning rate scaling and
training duration, as a basis for an experimental comparison of test
performance for different mini-batch sizes. We adopt a learning rate that
corresponds to a constant average weight update per gradient calculation (i.e.,
per unit cost of computation), and point out that this results in a variance of
the weight updates that increases linearly with the mini-batch size $m$.
  The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet
datasets show that increasing the mini-batch size progressively reduces the
range of learning rates that provide stable convergence and acceptable test
performance. On the other hand, small mini-batch sizes provide more up-to-date
gradient calculations, which yields more stable and reliable training. The best
performance has been consistently obtained for mini-batch sizes between $m = 2$
and $m = 32$, which contrasts with recent work advocating the use of mini-batch
sizes in the thousands.
","[{'version': 'v1', 'created': 'Fri, 20 Apr 2018 13:44:12 GMT'}]",2018-04-23,"[['Masters', 'Dominic', ''], ['Luschi', 'Carlo', '']]",2018,4,23,Revisiting Small Batch Training for Deep Neural Networks,23,zscore: 4.942135,"Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput.   In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size $m$.   The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between $m = 2$ and $m = 32$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.",Image ,,Learning,Aspects of DL,Evaluation,,,
88,1804.08328,Amir Zamir,"Amir Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra
  Malik, Silvio Savarese",Taskonomy: Disentangling Task Transfer Learning,"CVPR 2018 (Oral). See project website and live demos at
  http://taskonomy.vision/",,,,cs.CV cs.AI cs.LG cs.NE cs.RO,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Do visual tasks have a relationship, or are they unrelated? For instance,
could having surface normals simplify estimating the depth of an image?
Intuition answers these questions positively, implying existence of a structure
among visual tasks. Knowing this structure has notable values; it is the
concept underlying transfer learning and provides a principled way for
identifying redundancies across tasks, e.g., to seamlessly reuse supervision
among related tasks or solve many tasks in one system without piling up the
complexity.
  We proposes a fully computational approach for modeling the structure of
space of visual tasks. This is done via finding (first and higher-order)
transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D,
and semantic tasks in a latent space. The product is a computational taxonomic
map for task transfer learning. We study the consequences of this structure,
e.g. nontrivial emerged relationships, and exploit them to reduce the demand
for labeled data. For example, we show that the total number of labeled
datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3
(compared to training independently) while keeping the performance nearly the
same. We provide a set of tools for computing and probing this taxonomical
structure including a solver that users can employ to devise efficient
supervision policies for their use cases.
","[{'version': 'v1', 'created': 'Mon, 23 Apr 2018 10:46:28 GMT'}]",2018-04-24,"[['Zamir', 'Amir', ''], ['Sax', 'Alexander', ''], ['Shen', 'William', ''], ['Guibas', 'Leonidas', ''], ['Malik', 'Jitendra', ''], ['Savarese', 'Silvio', '']]",2018,4,24,Taskonomy: Disentangling Task Transfer Learning,18,zscore: 4.012240,"Do visual tasks have a relationship, or are they unrelated? For instance, could having surface normals simplify estimating the depth of an image? Intuition answers these questions positively, implying existence of a structure among visual tasks. Knowing this structure has notable values; it is the concept underlying transfer learning and provides a principled way for identifying redundancies across tasks, e.g., to seamlessly reuse supervision among related tasks or solve many tasks in one system without piling up the complexity.   We proposes a fully computational approach for modeling the structure of space of visual tasks. This is done via finding (first and higher-order) transfer learning dependencies across a dictionary of twenty six 2D, 2.5D, 3D, and semantic tasks in a latent space. The product is a computational taxonomic map for task transfer learning. We study the consequences of this structure, e.g. nontrivial emerged relationships, and exploit them to reduce the demand for labeled data. For example, we show that the total number of labeled datapoints needed for solving a set of 10 tasks can be reduced by roughly 2/3 (compared to training independently) while keeping the performance nearly the same. We provide a set of tools for computing and probing this taxonomical structure including a solver that users can employ to devise efficient supervision policies for their use cases.",Image,,Learning,Transfer Learning,Fewer data,,,
89,1804.08617,Matthew W. Hoffman,"Gabriel Barth-Maron, Matthew W. Hoffman, David Budden, Will Dabney,
  Dan Horgan, Dhruva TB, Alistair Muldal, Nicolas Heess, Timothy Lillicrap",Distributed Distributional Deterministic Policy Gradients,,,,,cs.LG cs.AI stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  This work adopts the very successful distributional perspective on
reinforcement learning and adapts it to the continuous control setting. We
combine this within a distributed framework for off-policy learning in order to
develop what we call the Distributed Distributional Deep Deterministic Policy
Gradient algorithm, D4PG. We also combine this technique with a number of
additional, simple improvements such as the use of $N$-step returns and
prioritized experience replay. Experimentally we examine the contribution of
each of these individual components, and show how they interact, as well as
their combined contributions. Our results show that across a wide variety of
simple control tasks, difficult manipulation tasks, and a set of hard
obstacle-based locomotion tasks the D4PG algorithm achieves state of the art
performance.
","[{'version': 'v1', 'created': 'Mon, 23 Apr 2018 11:57:21 GMT'}]",2018-04-25,"[['Barth-Maron', 'Gabriel', ''], ['Hoffman', 'Matthew W.', ''], ['Budden', 'David', ''], ['Dabney', 'Will', ''], ['Horgan', 'Dan', ''], ['TB', 'Dhruva', ''], ['Muldal', 'Alistair', ''], ['Heess', 'Nicolas', ''], ['Lillicrap', 'Timothy', '']]",2018,4,25,Distributed Distributional Deterministic Policy Gradients,20,zscore: 4.505308,"This work adopts the very successful distributional perspective on reinforcement learning and adapts it to the continuous control setting. We combine this within a distributed framework for off-policy learning in order to develop what we call the Distributed Distributional Deep Deterministic Policy Gradient algorithm, D4PG. We also combine this technique with a number of additional, simple improvements such as the use of $N$-step returns and prioritized experience replay. Experimentally we examine the contribution of each of these individual components, and show how they interact, as well as their combined contributions. Our results show that across a wide variety of simple control tasks, difficult manipulation tasks, and a set of hard obstacle-based locomotion tasks the D4PG algorithm achieves state of the art performance.",Locomotion,,Reinforcement Learning,,Better accuracy,,,
90,1804.08838,Chunyuan Li,"Chunyuan Li, Heerad Farkhoor, Rosanne Liu, Jason Yosinski",Measuring the Intrinsic Dimension of Objective Landscapes,Published in ICLR 2018,,,,cs.LG cs.NE stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Many recently trained neural networks employ large numbers of parameters to
achieve good performance. One may intuitively use the number of parameters
required as a rough gauge of the difficulty of a problem. But how accurate are
such notions? How many parameters are really needed? In this paper we attempt
to answer this question by training networks not in their native parameter
space, but instead in a smaller, randomly oriented subspace. We slowly increase
the dimension of this subspace, note at which dimension solutions first appear,
and define this to be the intrinsic dimension of the objective landscape. The
approach is simple to implement, computationally tractable, and produces
several suggestive conclusions. Many problems have smaller intrinsic dimensions
than one might suspect, and the intrinsic dimension for a given dataset varies
little across a family of models with vastly different sizes. This latter
result has the profound implication that once a parameter space is large enough
to solve a problem, extra parameters serve directly to increase the
dimensionality of the solution manifold. Intrinsic dimension allows some
quantitative comparison of problem difficulty across supervised, reinforcement,
and other types of learning where we conclude, for example, that solving the
inverted pendulum problem is 100 times easier than classifying digits from
MNIST, and playing Atari Pong from pixels is about as hard as classifying
CIFAR-10. In addition to providing new cartography of the objective landscapes
wandered by parameterized models, the method is a simple technique for
constructively obtaining an upper bound on the minimum description length of a
solution. A byproduct of this construction is a simple approach for compressing
networks, in some cases by more than 100 times.
","[{'version': 'v1', 'created': 'Tue, 24 Apr 2018 04:29:10 GMT'}]",2018-04-25,"[['Li', 'Chunyuan', ''], ['Farkhoor', 'Heerad', ''], ['Liu', 'Rosanne', ''], ['Yosinski', 'Jason', '']]",2018,4,25,Measuring the Intrinsic Dimension of Objective Landscapes,22,zscore: 4.980117,"Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.",Theoretical,,Learning,Aspects of DL,Evaluation,,,
91,1804.09541,Adams Wei Yu,"Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen,
  Mohammad Norouzi, Quoc V. Le","QANet: Combining Local Convolution with Global Self-Attention for
  Reading Comprehension",Published as full paper in ICLR 2018,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Current end-to-end machine reading and question answering (Q\&A) models are
primarily based on recurrent neural networks (RNNs) with attention. Despite
their success, these models are often slow for both training and inference due
to the sequential nature of RNNs. We propose a new Q\&A architecture called
QANet, which does not require recurrent networks: Its encoder consists
exclusively of convolution and self-attention, where convolution models local
interactions and self-attention models global interactions. On the SQuAD
dataset, our model is 3x to 13x faster in training and 4x to 9x faster in
inference, while achieving equivalent accuracy to recurrent models. The
speed-up gain allows us to train the model with much more data. We hence
combine our model with data generated by backtranslation from a neural machine
translation model. On the SQuAD dataset, our single model, trained with
augmented data, achieves 84.6 F1 score on the test set, which is significantly
better than the best published F1 score of 81.8.
","[{'version': 'v1', 'created': 'Mon, 23 Apr 2018 11:33:43 GMT'}]",2018-04-26,"[['Yu', 'Adams Wei', ''], ['Dohan', 'David', ''], ['Luong', 'Minh-Thang', ''], ['Zhao', 'Rui', ''], ['Chen', 'Kai', ''], ['Norouzi', 'Mohammad', ''], ['Le', 'Quoc V.', '']]",2018,4,26,QANet: Combining Local Convolution with Global Self-Attention for   Reading Comprehension,52,zscore: 12.394387,"Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",Text,QA,Deep Learning,Architectures (Attention),Faster,,,
92,1805.02242,Wenjie Ruan,"Wenjie Ruan, Xiaowei Huang, Marta Kwiatkowska",Reachability Analysis of Deep Neural Networks with Provable Guarantees,"This is the long version of the conference paper accepted in
  IJCAI-2018. Github: https://github.com/TrustAI/DeepGO",,,,cs.LG cs.CV stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Verifying correctness of deep neural networks (DNNs) is challenging. We study
a generic reachability problem for feed-forward DNNs which, for a given set of
inputs to the network and a Lipschitz-continuous function over its outputs,
computes the lower and upper bound on the function values. Because the network
and the function are Lipschitz continuous, all values in the interval between
the lower and upper bound are reachable. We show how to obtain the safety
verification problem, the output range analysis problem and a robustness
measure by instantiating the reachability problem. We present a novel algorithm
based on adaptive nested optimisation to solve the reachability problem. The
technique has been implemented and evaluated on a range of DNNs, demonstrating
its efficiency, scalability and ability to handle a broader class of networks
than state-of-the-art verification approaches.
","[{'version': 'v1', 'created': 'Sun, 6 May 2018 16:33:52 GMT'}]",2018-05-08,"[['Ruan', 'Wenjie', ''], ['Huang', 'Xiaowei', ''], ['Kwiatkowska', 'Marta', '']]",2018,5,8,Reachability Analysis of Deep Neural Networks with Provable Guarantees,11,zscore: 3.641909,"Verifying correctness of deep neural networks (DNNs) is challenging. We study a generic reachability problem for feed-forward DNNs which, for a given set of inputs to the network and a Lipschitz-continuous function over its outputs, computes the lower and upper bound on the function values. Because the network and the function are Lipschitz continuous, all values in the interval between the lower and upper bound are reachable. We show how to obtain the safety verification problem, the output range analysis problem and a robustness measure by instantiating the reachability problem. We present a novel algorithm based on adaptive nested optimisation to solve the reachability problem. The technique has been implemented and evaluated on a range of DNNs, demonstrating its efficiency, scalability and ability to handle a broader class of networks than state-of-the-art verification approaches.",Theoretical,,Learning,Aspects of DL,Scalability,,,
93,1805.03294,Albert Zeyer,"Albert Zeyer, and Kazuki Irie, and Ralf Schl\""uter, and Hermann Ney",Improved training of end-to-end attention models for speech recognition,submitted to Interspeech 2018,,10.21437/Interspeech.2018-1616,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Sequence-to-sequence attention-based models on subword units allow simple
open-vocabulary end-to-end speech recognition. In this work, we show that such
models can achieve competitive results on the Switchboard 300h and LibriSpeech
1000h tasks. In particular, we report the state-of-the-art word error rates
(WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets
of LibriSpeech. We introduce a new pretraining scheme by starting with a high
time reduction factor and lowering it during training, which is crucial both
for convergence and final performance. In some experiments, we also use an
auxiliary CTC loss function to help the convergence. In addition, we train long
short-term memory (LSTM) language models on subword units. By shallow fusion,
we report up to 27% relative improvements in WER over the attention baseline
without a language model.
","[{'version': 'v1', 'created': 'Tue, 8 May 2018 21:27:04 GMT'}]",2019-08-06,"[['Zeyer', 'Albert', ''], ['Irie', 'Kazuki', ''], ['Schlüter', 'Ralf', ''], ['Ney', 'Hermann', '']]",2019,8,6,Improved training of end-to-end attention models for speech recognition,19,zscore: 7.140431,"Sequence-to-sequence attention-based models on subword units allow simple open-vocabulary end-to-end speech recognition. In this work, we show that such models can achieve competitive results on the Switchboard 300h and LibriSpeech 1000h tasks. In particular, we report the state-of-the-art word error rates (WER) of 3.54% on the dev-clean and 3.82% on the test-clean evaluation subsets of LibriSpeech. We introduce a new pretraining scheme by starting with a high time reduction factor and lowering it during training, which is crucial both for convergence and final performance. In some experiments, we also use an auxiliary CTC loss function to help the convergence. In addition, we train long short-term memory (LSTM) language models on subword units. By shallow fusion, we report up to 27% relative improvements in WER over the attention baseline without a language model.",Speech,Recognition,Generation,Seq2Seq,Better accuracy,,,
94,1805.03620,Sebastian Ruder,"Anders S{\o}gaard, Sebastian Ruder, Ivan Vuli\'c",On the Limitations of Unsupervised Bilingual Dictionary Induction,ACL 2018,,,,cs.CL cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Unsupervised machine translation---i.e., not assuming any cross-lingual
supervision signal, whether a dictionary, translations, or comparable
corpora---seems impossible, but nevertheless, Lample et al. (2018) recently
proposed a fully unsupervised machine translation (MT) model. The model relies
heavily on an adversarial, unsupervised alignment of word embedding spaces for
bilingual dictionary induction (Conneau et al., 2018), which we examine here.
Our results identify the limitations of current unsupervised MT: unsupervised
bilingual dictionary induction performs much worse on morphologically rich
languages that are not dependent marking, when monolingual corpora from
different domains or different embedding algorithms are used. We show that a
simple trick, exploiting a weak supervision signal from identical words,
enables more robust induction, and establish a near-perfect correlation between
unsupervised bilingual dictionary induction performance and a previously
unexplored graph similarity metric.
","[{'version': 'v1', 'created': 'Wed, 9 May 2018 17:08:03 GMT'}]",2018-05-10,"[['Søgaard', 'Anders', ''], ['Ruder', 'Sebastian', ''], ['Vulić', 'Ivan', '']]",2018,5,10,On the Limitations of Unsupervised Bilingual Dictionary Induction,12,zscore: 4.464601,"Unsupervised machine translation---i.e., not assuming any cross-lingual supervision signal, whether a dictionary, translations, or comparable corpora---seems impossible, but nevertheless, Lample et al. (2018) recently proposed a fully unsupervised machine translation (MT) model. The model relies heavily on an adversarial, unsupervised alignment of word embedding spaces for bilingual dictionary induction (Conneau et al., 2018), which we examine here. Our results identify the limitations of current unsupervised MT: unsupervised bilingual dictionary induction performs much worse on morphologically rich languages that are not dependent marking, when monolingual corpora from different domains or different embedding algorithms are used. We show that a simple trick, exploiting a weak supervision signal from identical words, enables more robust induction, and establish a near-perfect correlation between unsupervised bilingual dictionary induction performance and a previously unexplored graph similarity metric.",Text,,Representation Learning,,Better accuracy,,,
95,1805.03643,Fei Tian,"Yang Fan, Fei Tian, Tao Qin, Xiang-Yang Li, Tie-Yan Liu",Learning to Teach,ICLR 2018,,,,cs.LG cs.AI,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Teaching plays a very important role in our society, by spreading human
knowledge and educating our next generations. A good teacher will select
appropriate teaching materials, impact suitable methodologies, and set up
targeted examinations, according to the learning behaviors of the students. In
the field of artificial intelligence, however, one has not fully explored the
role of teaching, and pays most attention to machine \emph{learning}. In this
paper, we argue that equal attention, if not more, should be paid to teaching,
and furthermore, an optimization framework (instead of heuristics) should be
used to obtain good teaching strategies. We call this approach `learning to
teach'. In the approach, two intelligent agents interact with each other: a
student model (which corresponds to the learner in traditional machine learning
algorithms), and a teacher model (which determines the appropriate data, loss
function, and hypothesis space to facilitate the training of the student
model). The teacher model leverages the feedback from the student model to
optimize its own teaching strategies by means of reinforcement learning, so as
to achieve teacher-student co-evolution. To demonstrate the practical value of
our proposed approach, we take the training of deep neural networks (DNN) as an
example, and show that by using the learning to teach techniques, we are able
to use much less training data and fewer iterations to achieve almost the same
accuracy for different kinds of DNN models (e.g., multi-layer perceptron,
convolutional neural networks and recurrent neural networks) under various
machine learning tasks (e.g., image classification and text understanding).
","[{'version': 'v1', 'created': 'Wed, 9 May 2018 04:41:26 GMT'}]",2018-05-11,"[['Fan', 'Yang', ''], ['Tian', 'Fei', ''], ['Qin', 'Tao', ''], ['Li', 'Xiang-Yang', ''], ['Liu', 'Tie-Yan', '']]",2018,5,11,Learning to Teach,11,zscore: 4.051175,"Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach `learning to teach'. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).",Various,Image+Text,Reinforcement Learning,,Fewer data,,,
96,1805.03989,Junyang Lin,"Junyang Lin, Xu Sun, Shuming Ma and Qi Su",Global Encoding for Abstractive Summarization,Accepted by ACL 2018,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  In neural abstractive summarization, the conventional sequence-to-sequence
(seq2seq) model often suffers from repetition and semantic irrelevance. To
tackle the problem, we propose a global encoding framework, which controls the
information flow from the encoder to the decoder based on the global
information of the source context. It consists of a convolutional gated unit to
perform global encoding to improve the representations of the source-side
information. Evaluations on the LCSTS and the English Gigaword both demonstrate
that our model outperforms the baseline models, and the analysis shows that our
model is capable of reducing repetition.
","[{'version': 'v1', 'created': 'Thu, 10 May 2018 14:11:51 GMT'}, {'version': 'v2', 'created': 'Sun, 10 Jun 2018 15:29:18 GMT'}]",2018-06-14,"[['Lin', 'Junyang', ''], ['Sun', 'Xu', ''], ['Ma', 'Shuming', ''], ['Su', 'Qi', '']]",2018,6,14,Global Encoding for Abstractive Summarization,10,zscore: 3.566655,"In neural abstractive summarization, the conventional sequence-to-sequence (seq2seq) model often suffers from repetition and semantic irrelevance. To tackle the problem, we propose a global encoding framework, which controls the information flow from the encoder to the decoder based on the global information of the source context. It consists of a convolutional gated unit to perform global encoding to improve the representations of the source-side information. Evaluations on the LCSTS and the English Gigaword both demonstrate that our model outperforms the baseline models, and the analysis shows that our model is capable of reducing repetition.",Text,Summarization,Generation,"Summarization, Seq2Seq",Better accuracy,,,
97,1805.04770,Tommaso Furlanello,"Tommaso Furlanello, Zachary C. Lipton, Michael Tschannen, Laurent Itti
  and Anima Anandkumar",Born Again Neural Networks,Published @ICML 2018,,,,stat.ML cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Knowledge distillation (KD) consists of transferring knowledge from one
machine learning model (the teacher}) to another (the student). Commonly, the
teacher is a high-capacity model with formidable performance, while the student
is more compact. By transferring knowledge, one hopes to benefit from the
student's compactness. %we desire a compact model with performance close to the
teacher's. We study KD from a new perspective: rather than compressing models,
we train students parameterized identically to their teachers. Surprisingly,
these {Born-Again Networks (BANs), outperform their teachers significantly,
both on computer vision and language modeling tasks. Our experiments with BANs
based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10
(3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional
experiments explore two distillation objectives: (i) Confidence-Weighted by
Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP).
Both methods elucidate the essential components of KD, demonstrating a role of
the teacher outputs on both predicted and non-predicted classes. We present
experiments with students of various capacities, focusing on the under-explored
case where students overpower teachers. Our experiments show significant
advantages from transferring knowledge between DenseNets and ResNets in either
direction.
","[{'version': 'v1', 'created': 'Sat, 12 May 2018 19:48:50 GMT'}, {'version': 'v2', 'created': 'Fri, 29 Jun 2018 10:46:28 GMT'}]",2018-07-02,"[['Furlanello', 'Tommaso', ''], ['Lipton', 'Zachary C.', ''], ['Tschannen', 'Michael', ''], ['Itti', 'Laurent', ''], ['Anandkumar', 'Anima', '']]",2018,7,2,Born Again Neural Networks,15,zscore: 4.146757,"Knowledge distillation (KD) consists of transferring knowledge from one machine learning model (the teacher}) to another (the student). Commonly, the teacher is a high-capacity model with formidable performance, while the student is more compact. By transferring knowledge, one hopes to benefit from the student's compactness. %we desire a compact model with performance close to the teacher's. We study KD from a new perspective: rather than compressing models, we train students parameterized identically to their teachers. Surprisingly, these {Born-Again Networks (BANs), outperform their teachers significantly, both on computer vision and language modeling tasks. Our experiments with BANs based on DenseNets demonstrate state-of-the-art performance on the CIFAR-10 (3.5%) and CIFAR-100 (15.5%) datasets, by validation error. Additional experiments explore two distillation objectives: (i) Confidence-Weighted by Teacher Max (CWTM) and (ii) Dark Knowledge with Permuted Predictions (DKPP). Both methods elucidate the essential components of KD, demonstrating a role of the teacher outputs on both predicted and non-predicted classes. We present experiments with students of various capacities, focusing on the under-explored case where students overpower teachers. Our experiments show significant advantages from transferring knowledge between DenseNets and ResNets in either direction.",Image ,,Distillation,,Better accuracy,,,
98,1805.06297,Mikel Artetxe,"Mikel Artetxe, Gorka Labaka, Eneko Agirre","A robust self-learning method for fully unsupervised cross-lingual
  mappings of word embeddings",ACL 2018,,,,cs.CL cs.AI cs.LG,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  Recent work has managed to learn cross-lingual word embeddings without
parallel data by mapping monolingual embeddings to a shared space through
adversarial training. However, their evaluation has focused on favorable
conditions, using comparable corpora or closely-related languages, and we show
that they often fail in more realistic scenarios. This work proposes an
alternative approach based on a fully unsupervised initialization that
explicitly exploits the structural similarity of the embeddings, and a robust
self-learning algorithm that iteratively improves this solution. Our method
succeeds in all tested scenarios and obtains the best published results in
standard datasets, even surpassing previous supervised systems. Our
implementation is released as an open source project at
https://github.com/artetxem/vecmap
","[{'version': 'v1', 'created': 'Wed, 16 May 2018 13:23:48 GMT'}, {'version': 'v2', 'created': 'Thu, 17 May 2018 17:21:53 GMT'}]",2018-05-18,"[['Artetxe', 'Mikel', ''], ['Labaka', 'Gorka', ''], ['Agirre', 'Eneko', '']]",2018,5,18,A robust self-learning method for fully unsupervised cross-lingual   mappings of word embeddings,15,zscore: 4.194759,"Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap",Text,,Representation Learning,,Better accuracy,,,
99,1805.07848,Yaniv Taigman,"Noam Mor, Lior Wolf, Adam Polyak, Yaniv Taigman",A Universal Music Translation Network,,,,,cs.SD cs.AI cs.LG stat.ML,http://arxiv.org/licenses/nonexclusive-distrib/1.0/,"  We present a method for translating music across musical instruments, genres,
and styles. This method is based on a multi-domain wavenet autoencoder, with a
shared encoder and a disentangled latent space that is trained end-to-end on
waveforms. Employing a diverse training dataset and large net capacity, the
domain-independent encoder allows us to translate even from musical domains
that were not seen during training. The method is unsupervised and does not
rely on supervision in the form of matched samples between domains or musical
transcriptions. We evaluate our method on NSynth, as well as on a dataset
collected from professional musicians, and achieve convincing translations,
even when translating from whistling, potentially enabling the creation of
instrumental music by untrained humans.
","[{'version': 'v1', 'created': 'Mon, 21 May 2018 00:20:03 GMT'}, {'version': 'v2', 'created': 'Wed, 23 May 2018 17:23:54 GMT'}]",2018-05-24,"[['Mor', 'Noam', ''], ['Wolf', 'Lior', ''], ['Polyak', 'Adam', ''], ['Taigman', 'Yaniv', '']]",2018,5,24,A Universal Music Translation Network,13,zscore: 3.768620,"We present a method for translating music across musical instruments, genres, and styles. This method is based on a multi-domain wavenet autoencoder, with a shared encoder and a disentangled latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the domain-independent encoder allows us to translate even from musical domains that were not seen during training. The method is unsupervised and does not rely on supervision in the form of matched samples between domains or musical transcriptions. We evaluate our method on NSynth, as well as on a dataset collected from professional musicians, and achieve convincing translations, even when translating from whistling, potentially enabling the creation of instrumental music by untrained humans.",Speech,Music,Generation,Seq2Seq,Fewer data,,,
